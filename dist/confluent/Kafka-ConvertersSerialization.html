<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Kafka连接器深度解读之转换器和序列化 | Apache Ignite中文站</title>
    <meta name="description" content="Ignite内存计算平台的中文文档及相关资料">
    <meta http-equiv="cache-control" content="no-store,no-cache,must-revalidate">
  <meta http-equiv="expires" content="0">
  <link rel="shortcut icon" type="image/x-icon" href="https://ignite.apache.org/favicon.ico">
  <script src="https://hm.baidu.com/hm.js?0ac821229cf1f2e3f580c7eb6b5cd3a5"></script>
    
    <link rel="preload" href="/assets/css/0.styles.f86987ee.css" as="style"><link rel="preload" href="/assets/js/app.b4afcc0e.js" as="script"><link rel="preload" href="/assets/js/8.447d2fe0.js" as="script"><link rel="prefetch" href="/assets/js/10.8e8203a7.js"><link rel="prefetch" href="/assets/js/100.64422fc2.js"><link rel="prefetch" href="/assets/js/101.1179ca52.js"><link rel="prefetch" href="/assets/js/102.f0fbcad1.js"><link rel="prefetch" href="/assets/js/103.6233b7f6.js"><link rel="prefetch" href="/assets/js/104.f2ec6a89.js"><link rel="prefetch" href="/assets/js/105.b5e05c6c.js"><link rel="prefetch" href="/assets/js/106.dabac401.js"><link rel="prefetch" href="/assets/js/107.8d478bfe.js"><link rel="prefetch" href="/assets/js/108.333839a6.js"><link rel="prefetch" href="/assets/js/109.c97593f3.js"><link rel="prefetch" href="/assets/js/11.202ada0f.js"><link rel="prefetch" href="/assets/js/110.c55987bd.js"><link rel="prefetch" href="/assets/js/111.1f5539d0.js"><link rel="prefetch" href="/assets/js/112.00dd9134.js"><link rel="prefetch" href="/assets/js/113.f456a29b.js"><link rel="prefetch" href="/assets/js/114.1111f87c.js"><link rel="prefetch" href="/assets/js/115.99d1fca5.js"><link rel="prefetch" href="/assets/js/116.5827665c.js"><link rel="prefetch" href="/assets/js/117.e4b1f5d2.js"><link rel="prefetch" href="/assets/js/12.da43ae1b.js"><link rel="prefetch" href="/assets/js/13.2a6fbeb4.js"><link rel="prefetch" href="/assets/js/14.65a7c9c4.js"><link rel="prefetch" href="/assets/js/15.722169a0.js"><link rel="prefetch" href="/assets/js/16.571393f5.js"><link rel="prefetch" href="/assets/js/17.899c831e.js"><link rel="prefetch" href="/assets/js/18.ea6a5616.js"><link rel="prefetch" href="/assets/js/19.040f7789.js"><link rel="prefetch" href="/assets/js/2.0b8aaecb.js"><link rel="prefetch" href="/assets/js/20.f628ebbf.js"><link rel="prefetch" href="/assets/js/21.73e1175c.js"><link rel="prefetch" href="/assets/js/22.9266b3a6.js"><link rel="prefetch" href="/assets/js/23.481de5ba.js"><link rel="prefetch" href="/assets/js/24.b45f18e6.js"><link rel="prefetch" href="/assets/js/25.645e0510.js"><link rel="prefetch" href="/assets/js/26.e630c248.js"><link rel="prefetch" href="/assets/js/27.40257737.js"><link rel="prefetch" href="/assets/js/28.f83517b5.js"><link rel="prefetch" href="/assets/js/29.5c7de173.js"><link rel="prefetch" href="/assets/js/3.d3c59160.js"><link rel="prefetch" href="/assets/js/30.b4412f36.js"><link rel="prefetch" href="/assets/js/31.437c8758.js"><link rel="prefetch" href="/assets/js/32.94fbab9a.js"><link rel="prefetch" href="/assets/js/33.4e198842.js"><link rel="prefetch" href="/assets/js/34.b5bbd597.js"><link rel="prefetch" href="/assets/js/35.cbeee8fd.js"><link rel="prefetch" href="/assets/js/36.6957cca2.js"><link rel="prefetch" href="/assets/js/37.d7bf8dbe.js"><link rel="prefetch" href="/assets/js/38.762b85ee.js"><link rel="prefetch" href="/assets/js/39.2d61f2a1.js"><link rel="prefetch" href="/assets/js/4.10c7f2ba.js"><link rel="prefetch" href="/assets/js/40.39c83aec.js"><link rel="prefetch" href="/assets/js/41.113a0d66.js"><link rel="prefetch" href="/assets/js/42.22ef9479.js"><link rel="prefetch" href="/assets/js/43.cbb0a84c.js"><link rel="prefetch" href="/assets/js/44.025632f3.js"><link rel="prefetch" href="/assets/js/45.25565275.js"><link rel="prefetch" href="/assets/js/46.f78db23a.js"><link rel="prefetch" href="/assets/js/47.5eb807f5.js"><link rel="prefetch" href="/assets/js/48.aab65a89.js"><link rel="prefetch" href="/assets/js/49.04294765.js"><link rel="prefetch" href="/assets/js/5.8a81e9fc.js"><link rel="prefetch" href="/assets/js/50.5304fc2e.js"><link rel="prefetch" href="/assets/js/51.8100c7b5.js"><link rel="prefetch" href="/assets/js/52.96c34a93.js"><link rel="prefetch" href="/assets/js/53.743ead38.js"><link rel="prefetch" href="/assets/js/54.7fc8cc30.js"><link rel="prefetch" href="/assets/js/55.c60d725a.js"><link rel="prefetch" href="/assets/js/56.84092c9a.js"><link rel="prefetch" href="/assets/js/57.af7ab3f6.js"><link rel="prefetch" href="/assets/js/58.ad40a388.js"><link rel="prefetch" href="/assets/js/59.e09ef91e.js"><link rel="prefetch" href="/assets/js/6.91205a0f.js"><link rel="prefetch" href="/assets/js/60.75589dfc.js"><link rel="prefetch" href="/assets/js/61.338a2e5a.js"><link rel="prefetch" href="/assets/js/62.0dc2b7a4.js"><link rel="prefetch" href="/assets/js/63.15bcd9a3.js"><link rel="prefetch" href="/assets/js/64.ee5dfcb3.js"><link rel="prefetch" href="/assets/js/65.690cb0d4.js"><link rel="prefetch" href="/assets/js/66.ef7fc66a.js"><link rel="prefetch" href="/assets/js/67.6a722259.js"><link rel="prefetch" href="/assets/js/68.dc286622.js"><link rel="prefetch" href="/assets/js/69.99de0332.js"><link rel="prefetch" href="/assets/js/7.3c31d9d0.js"><link rel="prefetch" href="/assets/js/70.64c87179.js"><link rel="prefetch" href="/assets/js/71.4fa1dabc.js"><link rel="prefetch" href="/assets/js/72.1320f18d.js"><link rel="prefetch" href="/assets/js/73.75b06019.js"><link rel="prefetch" href="/assets/js/74.5e39a5b8.js"><link rel="prefetch" href="/assets/js/75.8474449d.js"><link rel="prefetch" href="/assets/js/76.d9b1514f.js"><link rel="prefetch" href="/assets/js/77.9e4db3de.js"><link rel="prefetch" href="/assets/js/78.d8eb1a5d.js"><link rel="prefetch" href="/assets/js/79.2fb70321.js"><link rel="prefetch" href="/assets/js/80.501e0615.js"><link rel="prefetch" href="/assets/js/81.f47904ce.js"><link rel="prefetch" href="/assets/js/82.40ece1cf.js"><link rel="prefetch" href="/assets/js/83.3e991c38.js"><link rel="prefetch" href="/assets/js/84.f4ab1f22.js"><link rel="prefetch" href="/assets/js/85.ba522441.js"><link rel="prefetch" href="/assets/js/86.1e708ae1.js"><link rel="prefetch" href="/assets/js/87.f2cf4473.js"><link rel="prefetch" href="/assets/js/88.c6603ddf.js"><link rel="prefetch" href="/assets/js/89.2ea125b9.js"><link rel="prefetch" href="/assets/js/9.994821de.js"><link rel="prefetch" href="/assets/js/90.26cc2e16.js"><link rel="prefetch" href="/assets/js/91.63ff2709.js"><link rel="prefetch" href="/assets/js/92.e11925b5.js"><link rel="prefetch" href="/assets/js/93.0c6a5335.js"><link rel="prefetch" href="/assets/js/94.027eac0c.js"><link rel="prefetch" href="/assets/js/95.6ba5bc4f.js"><link rel="prefetch" href="/assets/js/96.7a7ec032.js"><link rel="prefetch" href="/assets/js/97.78833ca8.js"><link rel="prefetch" href="/assets/js/98.74cd85eb.js"><link rel="prefetch" href="/assets/js/99.e1b978bf.js">
    <link rel="stylesheet" href="/assets/css/0.styles.f86987ee.css">
  </head>
  <body>
    <div id="app" data-server-rendered="true"><div class="theme-container"><header class="navbar"><div class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/" class="home-link router-link-active"><!----> <span class="site-name">Apache Ignite中文站</span></a> <div class="links" style="max-width:nullpx;"><!----> <nav class="nav-links can-hide"><div class="nav-item"><a href="/" class="nav-link">首页</a></div><div class="nav-item"><a href="/doc/java/" class="nav-link">Java</a></div><div class="nav-item"><a href="/doc/cpp/" class="nav-link">C++</a></div><div class="nav-item"><a href="/doc/sql/" class="nav-link">SQL</a></div><div class="nav-item"><a href="/doc/integration/" class="nav-link">集成</a></div><div class="nav-item"><a href="/doc/spark/" class="nav-link">Ignite&amp;Spark</a></div><div class="nav-item"><a href="/doc/tools/" class="nav-link">工具</a></div><div class="nav-item"><a href="/doc/gridgain/" class="nav-link">商业版</a></div><div class="nav-item"><a href="https://my.oschina.net/liyuj" target="_blank" rel="noopener noreferrer" class="nav-link external">
  博客
  <svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></div><div class="nav-item"><a href="/confluent/" class="nav-link router-link-active">Confluent平台</a></div><div class="nav-item"><a href="https://www.zybuluo.com/liyuj/note/230739" target="_blank" rel="noopener noreferrer" class="nav-link external">
  历史版本
  <svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></div><div class="nav-item"><div class="dropdown-wrapper"><a class="dropdown-title"><span class="title">最新版本</span> <span class="arrow right"></span></a> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/confluent/Kafka-ConvertersSerialization.html" class="nav-link router-link-exact-active router-link-active">2.7.0</a></li><li class="dropdown-item"><!----> <a href="/doc/2.6.0/" class="nav-link">2.6.0</a></li></ul></div></div> <!----></nav></div></header> <div class="sidebar-mask"></div> <div class="sidebar"><nav class="nav-links"><div class="nav-item"><a href="/" class="nav-link">首页</a></div><div class="nav-item"><a href="/doc/java/" class="nav-link">Java</a></div><div class="nav-item"><a href="/doc/cpp/" class="nav-link">C++</a></div><div class="nav-item"><a href="/doc/sql/" class="nav-link">SQL</a></div><div class="nav-item"><a href="/doc/integration/" class="nav-link">集成</a></div><div class="nav-item"><a href="/doc/spark/" class="nav-link">Ignite&amp;Spark</a></div><div class="nav-item"><a href="/doc/tools/" class="nav-link">工具</a></div><div class="nav-item"><a href="/doc/gridgain/" class="nav-link">商业版</a></div><div class="nav-item"><a href="https://my.oschina.net/liyuj" target="_blank" rel="noopener noreferrer" class="nav-link external">
  博客
  <svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></div><div class="nav-item"><a href="/confluent/" class="nav-link router-link-active">Confluent平台</a></div><div class="nav-item"><a href="https://www.zybuluo.com/liyuj/note/230739" target="_blank" rel="noopener noreferrer" class="nav-link external">
  历史版本
  <svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></div><div class="nav-item"><div class="dropdown-wrapper"><a class="dropdown-title"><span class="title">最新版本</span> <span class="arrow right"></span></a> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/confluent/Kafka-ConvertersSerialization.html" class="nav-link router-link-exact-active router-link-active">2.7.0</a></li><li class="dropdown-item"><!----> <a href="/doc/2.6.0/" class="nav-link">2.6.0</a></li></ul></div></div> <!----></nav>  <ul class="sidebar-links"><li><a href="/confluent/" class="sidebar-link">Kafka连接器深度解读之JDBC源连接器</a></li><li><a href="/confluent/Kafka-ConvertersSerialization.html" class="active sidebar-link">Kafka连接器深度解读之转换器和序列化</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/confluent/Kafka-ConvertersSerialization.html#kafka消息只是字节" class="sidebar-link">Kafka消息只是字节</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/confluent/Kafka-ConvertersSerialization.html#序列化格式的选择" class="sidebar-link">序列化格式的选择</a></li><li class="sidebar-sub-header"><a href="/confluent/Kafka-ConvertersSerialization.html#如果使用json格式写入目标端，需要在主题中使用json格式么？" class="sidebar-link">如果使用JSON格式写入目标端，需要在主题中使用JSON格式么？</a></li></ul></li><li class="sidebar-sub-header"><a href="/confluent/Kafka-ConvertersSerialization.html#配置转换器" class="sidebar-link">配置转换器</a></li><li class="sidebar-sub-header"><a href="/confluent/Kafka-ConvertersSerialization.html#json和模式" class="sidebar-link">JSON和模式</a></li><li class="sidebar-sub-header"><a href="/confluent/Kafka-ConvertersSerialization.html#常见错误" class="sidebar-link">常见错误</a></li><li class="sidebar-sub-header"><a href="/confluent/Kafka-ConvertersSerialization.html#解决问题的提示" class="sidebar-link">解决问题的提示</a></li><li class="sidebar-sub-header"><a href="/confluent/Kafka-ConvertersSerialization.html#内部转换器" class="sidebar-link">内部转换器</a></li><li class="sidebar-sub-header"><a href="/confluent/Kafka-ConvertersSerialization.html#将模式应用于没有模式的消息" class="sidebar-link">将模式应用于没有模式的消息</a></li><li class="sidebar-sub-header"><a href="/confluent/Kafka-ConvertersSerialization.html#结论" class="sidebar-link">结论</a></li></ul></li><li><a href="/confluent/Kafka-ErrorHandlingDeadLetterQueues.html" class="sidebar-link">Kafka连接器深度解读之错误处理和死信队列</a></li></ul> </div> <div class="page"> <div class="content"><h1 id="kafka连接器深度解读之转换器和序列化"><a href="#kafka连接器深度解读之转换器和序列化" aria-hidden="true" class="header-anchor">#</a> Kafka连接器深度解读之转换器和序列化</h1> <p><a href="https://docs.confluent.io/current/connect/index.html" target="_self" rel="noopener noreferrer">Kafka连接器</a>是Apache Kafka®的一部分，提供数据存储与Kafka之间的流式集成。对于数据工程师来说，只需要使用JSON格式配置文件即可。目前已经有很多数据存储的连接器，仅举几例来说，包括<a href="https://www.confluent.io/connector/kafka-connect-jdbc/" target="_self" rel="noopener noreferrer">JDBC</a>，<a href="https://www.confluent.io/connector/kafka-connect-elasticsearch/" target="_self" rel="noopener noreferrer">Elasticsearch</a>，<a href="https://www.confluent.io/connector/kafka-connect-ibm-mq/" target="_self" rel="noopener noreferrer">IBM MQ</a>，<a href="https://www.confluent.io/connector/kafka-connect-s3/" target="_self" rel="noopener noreferrer">S3</a>和<a href="https://www.confluent.io/connector/bigquery-sink-connector/" target="_self" rel="noopener noreferrer">BigQuery</a>。</p> <p>对于开发者，Kafka连接器有丰富的<a href="https://docs.confluent.io/current/connect/javadocs/index.html" target="_self" rel="noopener noreferrer">API</a>，如有必要，可以<a href="https://docs.confluent.io/current/connect/devguide.html" target="_self" rel="noopener noreferrer">开发</a>自己的连接器。此外它还具有用于配置和管理连接器的<a href="https://docs.confluent.io/current/connect/references/restapi.html" target="_self" rel="noopener noreferrer">REST API</a>。</p> <p>Kafka连接器本身是模块化的，提供了非常强大的满足集成需求的方法，部分关键组件包括：</p> <ul><li>连接器：定义了一组如何与数据存储集成的JAR文件；</li> <li>转换器：处理数据的序列化和反序列化；</li> <li>变换：传输过程中的消息处理（可选）。</li></ul> <p>围绕Kafka连接器，常见的错误或者误解之一是数据的序列化，这是Kafka连接器通过转换器进行处理的，下面会介绍它们的工作机制，并说明一些常见问题如何处理。</p> <h2 id="kafka消息只是字节"><a href="#kafka消息只是字节" aria-hidden="true" class="header-anchor">#</a> Kafka消息只是字节</h2> <p>Kafka消息是按照主题进行组织的。每条消息都是一个键/值对，不过Kafka就需要这些。当数据在Kafka中存储时都只是字节，这使得Kafka可以适用于各种场景，但这也意味着开发者有责任决定如何对数据进行序列化。</p> <p>在配置Kafka连接器时，标准步骤的关键之一是序列化格式，要确保主题的读取方和写入方使用相同的序列化格式，否则会出现混乱和错误！</p> <p><img src="https://www.confluent.io/wp-content/uploads/Data-serialization-with-Kafka.png" alt></p> <p>常见的格式有很多，包括：</p> <ul><li>JSON;</li> <li>Avro;</li> <li>Protobuf;</li> <li>字符串分割（如CSV）。</li></ul> <p>每种格式都有优点和缺点。</p> <h3 id="序列化格式的选择"><a href="#序列化格式的选择" aria-hidden="true" class="header-anchor">#</a> 序列化格式的选择</h3> <p>选择序列化格式的一些原则包括：</p> <ul><li><strong>模式</strong>：很多时候数据都会有一个模式。可能不喜欢这个事实，但作为开发人员有责任保留和传播此模式，因为模式提供了<a href="https://www.infoq.com/presentations/contracts-streaming-microservices" target="_self" rel="noopener noreferrer">服务之间的契约</a>。某些消息格式（例如Avro和Protobuf）具有强大的模式支持，而其它消息格式支持较少（JSON）或根本没有（分隔字符串）；</li> <li><strong>生态系统兼容性</strong>：Avro是Confluent平台的一等公民，得到了<a href="https://www.confluent.io/confluent-schema-registry/" target="_self" rel="noopener noreferrer">Confluent模式注册表</a>、Kafka连接器、<a href="https://www.confluent.io/product/ksql/" target="_self" rel="noopener noreferrer">KSQL</a>等的原生支持。而Protobuf则依赖于部分功能支持的社区贡献；</li> <li><strong>消息大小</strong>：JSON是纯文本格式，消息大小依赖于Kafka本身的压缩配置，而Avro和Protobuf都是二进制格式，因此消息较小；</li> <li><strong>语言支持</strong>：Java体系对Avro有强大的支持，但如果应用不是基于Java的，那么可能会发现它不太容易处理。</li></ul> <h3 id="如果使用json格式写入目标端，需要在主题中使用json格式么？"><a href="#如果使用json格式写入目标端，需要在主题中使用json格式么？" aria-hidden="true" class="header-anchor">#</a> 如果使用JSON格式写入目标端，需要在主题中使用JSON格式么？</h3> <p>不需要，不管是从源端读取数据的格式，还是将数据写入外部存储，都不会影响Kafka中消息序列化的格式。</p> <p>Kafka中的连接器负责从源端（例如数据库）读取数据，并将其作为<a href="https://docs.confluent.io/current/connect/javadocs/index.html?org/apache/kafka/connect/data/SchemaAndValue.html&_ga=2.83434774.1251956415.1553320583-1542045317.1553320583" target="_self" rel="noopener noreferrer">数据的内部表示</a>传递给转换器,然后，Kafka中的转换器会将此源数据对象序列化到主题上。</p> <p>当使用Kafka连接器作为接收端时，正好相反，即转换器将来自主题的数据反序列化为内部表示，其会传递給连接器，然后使用指定方法写入目标端。</p> <p>这意味着可以在主题中比如以Avro格式保存数据，然后比如将其写入HDFS时，再指定<a href="https://docs.confluent.io/current/connect/kafka-connect-hdfs/configuration_options.html?&_ga=2.50083494.1251956415.1553320583-1542045317.1553320583#connector" target="_self" rel="noopener noreferrer">接收端连接器使用的格式</a>。</p> <h2 id="配置转换器"><a href="#配置转换器" aria-hidden="true" class="header-anchor">#</a> 配置转换器</h2> <p>Kafka连接器在工作节点级别使用默认的转换器配置，也可以在每个连接器上覆盖它。由于在整个流水线中使用相同的序列化格式通常是一个好的做法，所以通常只需在工作节点上配置转换器，而无需在连接器中指定。但是如果从其它主题中提取数据而它们使用不同的序列化格式时，就要在连接器配置中指定它，即使在连接器的配置中覆盖它，执行任务的还是那个转换器。</p> <p><strong>正确的连接器永远不会序列化/反序列化存储在Kafka中的消息，而是让配置的转换器完成这项工作。</strong></p> <p><img src="https://www.confluent.io/wp-content/uploads/Configuring-converters-with-Kafka-Connect.png" alt></p> <p>注意Kafka消息只是键/值字节对，因此需要使用<code>key.converter</code>和<code>value.converter</code>配置项为键和值指定转换器，某些情况下，可以为键和值指定不同的转换器。</p> <p><img src="https://www.confluent.io/wp-content/uploads/Configuring-converters-with-Kafka-Connect-1.png" alt></p> <p>下面是使用String转换器的示例，由于它只是一个字符串，数据没有模式，因此用于<code>value</code>并不是那么有用：</p> <div class="language- line-numbers-mode"><pre class="language-text"><code>&quot;key.converter&quot;: &quot;org.apache.kafka.connect.storage.StringConverter&quot;,
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>某些转换器有其它配置项，对于Avro，需要指定<code>模式注册表</code>，对于JSON，需要指定是否希望Kafka连接器将模式嵌入JSON本身。为转换器指定配置项时，要使用<code>key.converter.</code>或<code>value.converter.</code>前缀。例如要将Avro用于消息的内容，需要指定以下配置项：</p> <div class="language- line-numbers-mode"><pre class="language-text"><code>&quot;value.converter&quot;: &quot;io.confluent.connect.avro.AvroConverter&quot;,
&quot;value.converter.schema.registry.url&quot;: &quot;http://schema-registry:8081&quot;,
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>常见的转换器包括：</p> <ul><li>Avro：<a href="https://www.confluent.io/connector/kafka-connect-avro-converter/" target="_self" rel="noopener noreferrer">Confluent平台</a>的一部分</li></ul> <div class="language- line-numbers-mode"><pre class="language-text"><code>io.confluent.connect.avro.AvroConverter
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><ul><li>String：Apache Kafka的一部分</li></ul> <div class="language- line-numbers-mode"><pre class="language-text"><code>org.apache.kafka.connect.storage.StringConverter
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><ul><li>JSON：Apache Kafka的一部分</li></ul> <div class="language- line-numbers-mode"><pre class="language-text"><code>org.apache.kafka.connect.json.JsonConverter
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><ul><li>ByteArray：Apache Kafka的一部分</li></ul> <div class="language- line-numbers-mode"><pre class="language-text"><code>org.apache.kafka.connect.converters.ByteArrayConverter
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><ul><li>Protobuf：<a href="https://www.confluent.io/connector/kafka-connect-protobuf-converter/" target="_self" rel="noopener noreferrer">社区开源</a></li></ul> <div class="language- line-numbers-mode"><pre class="language-text"><code>com.blueapron.connect.protobuf.ProtobufConverter
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><h2 id="json和模式"><a href="#json和模式" aria-hidden="true" class="header-anchor">#</a> JSON和模式</h2> <p>虽然JSON默认不支持携带模式，但Kafka连接器确实支持嵌入模式的特定JSON格式。由于模式也包含在每个消息中，因此生成的数据大小可能会变大。</p> <p>如果正在配置Kafka源连接器并且希望Kafka连接器在写入Kafka的消息中包含模式，需要做如下的配置：</p> <div class="language- line-numbers-mode"><pre class="language-text"><code>value.converter=org.apache.kafka.connect.json.JsonConverter
value.converter.schemas.enable=true
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>最终向Kafka写入的消息大致如下，<code>schema</code>以及<code>payload</code>为JSON中的顶级元素：</p> <div class="language-json line-numbers-mode"><pre class="language-json"><code><span class="token punctuation">{</span>
  <span class="token property">&quot;schema&quot;</span><span class="token operator">:</span> <span class="token punctuation">{</span>
    <span class="token property">&quot;type&quot;</span><span class="token operator">:</span> <span class="token string">&quot;struct&quot;</span><span class="token punctuation">,</span>
    <span class="token property">&quot;fields&quot;</span><span class="token operator">:</span> <span class="token punctuation">[</span>
      <span class="token punctuation">{</span>
        <span class="token property">&quot;type&quot;</span><span class="token operator">:</span> <span class="token string">&quot;int64&quot;</span><span class="token punctuation">,</span>
        <span class="token property">&quot;optional&quot;</span><span class="token operator">:</span> <span class="token boolean">false</span><span class="token punctuation">,</span>
        <span class="token property">&quot;field&quot;</span><span class="token operator">:</span> <span class="token string">&quot;registertime&quot;</span>
      <span class="token punctuation">}</span><span class="token punctuation">,</span>
      <span class="token punctuation">{</span>
        <span class="token property">&quot;type&quot;</span><span class="token operator">:</span> <span class="token string">&quot;string&quot;</span><span class="token punctuation">,</span>
        <span class="token property">&quot;optional&quot;</span><span class="token operator">:</span> <span class="token boolean">false</span><span class="token punctuation">,</span>
        <span class="token property">&quot;field&quot;</span><span class="token operator">:</span> <span class="token string">&quot;userid&quot;</span>
      <span class="token punctuation">}</span><span class="token punctuation">,</span>
      <span class="token punctuation">{</span>
        <span class="token property">&quot;type&quot;</span><span class="token operator">:</span> <span class="token string">&quot;string&quot;</span><span class="token punctuation">,</span>
        <span class="token property">&quot;optional&quot;</span><span class="token operator">:</span> <span class="token boolean">false</span><span class="token punctuation">,</span>
        <span class="token property">&quot;field&quot;</span><span class="token operator">:</span> <span class="token string">&quot;regionid&quot;</span>
      <span class="token punctuation">}</span><span class="token punctuation">,</span>
      <span class="token punctuation">{</span>
        <span class="token property">&quot;type&quot;</span><span class="token operator">:</span> <span class="token string">&quot;string&quot;</span><span class="token punctuation">,</span>
        <span class="token property">&quot;optional&quot;</span><span class="token operator">:</span> <span class="token boolean">false</span><span class="token punctuation">,</span>
        <span class="token property">&quot;field&quot;</span><span class="token operator">:</span> <span class="token string">&quot;gender&quot;</span>
      <span class="token punctuation">}</span>
    <span class="token punctuation">]</span><span class="token punctuation">,</span>
    <span class="token property">&quot;optional&quot;</span><span class="token operator">:</span> <span class="token boolean">false</span><span class="token punctuation">,</span>
    <span class="token property">&quot;name&quot;</span><span class="token operator">:</span> <span class="token string">&quot;ksql.users&quot;</span>
  <span class="token punctuation">}</span><span class="token punctuation">,</span>
  <span class="token property">&quot;payload&quot;</span><span class="token operator">:</span> <span class="token punctuation">{</span>
    <span class="token property">&quot;registertime&quot;</span><span class="token operator">:</span> <span class="token number">1493819497170</span><span class="token punctuation">,</span>
    <span class="token property">&quot;userid&quot;</span><span class="token operator">:</span> <span class="token string">&quot;User_1&quot;</span><span class="token punctuation">,</span>
    <span class="token property">&quot;regionid&quot;</span><span class="token operator">:</span> <span class="token string">&quot;Region_5&quot;</span><span class="token punctuation">,</span>
    <span class="token property">&quot;gender&quot;</span><span class="token operator">:</span> <span class="token string">&quot;MALE&quot;</span>
  <span class="token punctuation">}</span>
<span class="token punctuation">}</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br><span class="line-number">27</span><br><span class="line-number">28</span><br><span class="line-number">29</span><br><span class="line-number">30</span><br><span class="line-number">31</span><br><span class="line-number">32</span><br><span class="line-number">33</span><br><span class="line-number">34</span><br><span class="line-number">35</span><br></div></div><p>请注意消息的大小，以及由内容与模式组成的消息的大小。考虑到在每条消息中都重复这一点，就会看到为什么像Avro这样的格式很有意义，因为模式是单独存储的，而消息只包含有效内容（并进行过压缩）。</p> <p>如果从一个Kafka主题中使用Kafka接收连接器消费JSON格式的数据，则需要了解数据中是否包含模式，如果包含，则要与上面的格式相同，而不能是一些任意的格式，那么配置如下：</p> <div class="language- line-numbers-mode"><pre class="language-text"><code>value.converter=org.apache.kafka.connect.json.JsonConverter
value.converter.schemas.enable=true
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>不过，如果使用的JSON数据没有<code>schema/payload</code>结构，像下面这样：</p> <div class="language-json line-numbers-mode"><pre class="language-json"><code><span class="token punctuation">{</span>
  <span class="token property">&quot;registertime&quot;</span><span class="token operator">:</span> <span class="token number">1489869013625</span><span class="token punctuation">,</span>
  <span class="token property">&quot;userid&quot;</span><span class="token operator">:</span> <span class="token string">&quot;User_1&quot;</span><span class="token punctuation">,</span>
  <span class="token property">&quot;regionid&quot;</span><span class="token operator">:</span> <span class="token string">&quot;Region_2&quot;</span><span class="token punctuation">,</span>
  <span class="token property">&quot;gender&quot;</span><span class="token operator">:</span> <span class="token string">&quot;OTHER&quot;</span>
<span class="token punctuation">}</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br></div></div><p>则必须通过配置通知Kafka连接器不要寻找模式，如下：</p> <div class="language- line-numbers-mode"><pre class="language-text"><code>value.converter=org.apache.kafka.connect.json.JsonConverter
value.converter.schemas.enable=false
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>和以前一样，要记住转换器配置项（此处<code>schemas.enable</code>）需要合适的前缀<code>key.converter</code>或<code>value.converter</code>。</p> <h2 id="常见错误"><a href="#常见错误" aria-hidden="true" class="header-anchor">#</a> 常见错误</h2> <p>如果Kafka连接器中转换器配置不正确，可能遇到以下一些常见错误。这些消息会出现在Kafka连接器配置的接收端中，因为这里是对存储在Kafka中的消息进行反序列化的点。转换器问题通常不会在源端发生，因为源端已经配置了序列化。其中每个都会导致连接器失败，开始的错误为：</p> <div class="language- line-numbers-mode"><pre class="language-text"><code>ERROR WorkerSinkTask{id=sink-file-users-json-noschema-01-0} Task threw an uncaught and unrecoverable exception (org.apache.kafka.connect.runtime.WorkerTask)
org.apache.kafka.connect.errors.ConnectException: Tolerance exceeded in error handler
   at org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator. execAndHandleError(RetryWithToleranceOperator.java:178)
   at org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator.execute (RetryWithToleranceOperator.java:104)
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><p>这个错误之后，会看到一个异常堆栈，其中描述了错误的原因。<strong>注意对于连接器中的任何严重错误，都会抛出上述错误，因此可能会看到与序列化无关的错误</strong>。要快速定位错误是由哪个错误配置导致的，可参考下表：</p> <p><img src="https://www.confluent.io/wp-content/uploads/Errors-from-misconfiguring-converters-in-Kafka-Connect.001.jpeg" alt></p> <p><strong>问题：使用JsonConverter读取非JSON格式数据</strong></p> <p>如果源端主题上有非JSON格式的数据，但是使用JsonConverter进行读取，就会看到：</p> <div class="language- line-numbers-mode"><pre class="language-text"><code>org.apache.kafka.connect.errors.DataException: Converting byte[] to Kafka Connect data failed due to serialization error:
…
org.apache.kafka.common.errors.SerializationException: java.io.CharConversionException: Invalid UTF-32 character 0x1cfa7e2 (above 0x0010ffff) at char #1, byte #7)
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p>这可能是因为源端主题以Avro或其它格式序列化引起的。</p> <p>解决方案：如果数据实际上是Avro格式，则需要按照如下方式修改Kafka连接器的接收端：</p> <div class="language- line-numbers-mode"><pre class="language-text"><code>&quot;value.converter&quot;: &quot;io.confluent.connect.avro.AvroConverter&quot;,
&quot;value.converter.schema.registry.url&quot;: &quot;http://schema-registry:8081&quot;,
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p><strong>或者</strong>，如果主题由Kafka连接器注入，也可以调整上游源端，让其输出JSON格式数据：</p> <div class="language- line-numbers-mode"><pre class="language-text"><code>&quot;value.converter&quot;: &quot;org.apache.kafka.connect.json.JsonConverter&quot;,
&quot;value.converter.schemas.enable&quot;: &quot;false&quot;,
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p><strong>问题：使用AvroConverter读取非Avro格式数据</strong></p> <p>这是最常见的错误，当尝试使用AvroConverter从非Avro格式的主题读取数据时，会发生这种情况，还包括使用非Confluent模式注册表的<a href="https://docs.confluent.io/5.0.0/schema-registry/docs/serializer-formatter.html?_ga=2.112366852.1251956415.1553320583-1542045317.1553320583#serializer" target="_self" rel="noopener noreferrer">Avro序列化器</a>写入的数据：</p> <div class="language- line-numbers-mode"><pre class="language-text"><code>org.apache.kafka.connect.errors.DataException: my-topic-name
at io.confluent.connect.avro.AvroConverter.toConnectData(AvroConverter.java:97)
…
org.apache.kafka.common.errors.SerializationException: Error deserializing Avro message for id -1
org.apache.kafka.common.errors.SerializationException: Unknown magic byte!
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><p>解决方案：检查源端主题的序列化格式，调整Kafka连接器接收端使用正确的转换器，或将上游格式修改为Avro（这样最好）。如果上游主题由Kafka连接器注入，也可以按如下方式配置源端连接器的转换器：</p> <div class="language- line-numbers-mode"><pre class="language-text"><code>&quot;value.converter&quot;: &quot;io.confluent.connect.avro.AvroConverter&quot;,
&quot;value.converter.schema.registry.url&quot;: &quot;http://schema-registry:8081&quot;,
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p><strong>问题：读取没有期望的schema/payload结构的JSON数据</strong></p> <p>如前所述，Kafka连接器支持包含有效内容和模式的特殊JSON格式消息结构，如果读取的JSON数据不是这样的结构，会有下面的错误：</p> <div class="language- line-numbers-mode"><pre class="language-text"><code>org.apache.kafka.connect.errors.DataException: JsonConverter with schemas.enable requires &quot;schema&quot; and &quot;payload&quot; fields and may not contain additional fields. If you are trying to deserialize plain JSON data, set schemas.enable=false in your converter configuration.
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>要知道，对于<code>schemas.enable=true</code>唯一有效的JSON结构是，<code>schema</code>和<code>payload</code>作为顶级元素（如上所示）。</p> <p>正如错误消息本身所述，如果只是简单的JSON数据，则应将连接器的配置更改为：</p> <div class="language- line-numbers-mode"><pre class="language-text"><code>&quot;value.converter&quot;: &quot;org.apache.kafka.connect.json.JsonConverter&quot;,
&quot;value.converter.schemas.enable&quot;: &quot;false&quot;,
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>如果要在数据中包含模式，要么切换到使用Avro（推荐），要么配置上游的Kafka连接器以在消息中包含模式：</p> <div class="language- line-numbers-mode"><pre class="language-text"><code>&quot;value.converter&quot;: &quot;org.apache.kafka.connect.json.JsonConverter&quot;,
&quot;value.converter.schemas.enable&quot;: &quot;true&quot;,
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><h2 id="解决问题的提示"><a href="#解决问题的提示" aria-hidden="true" class="header-anchor">#</a> 解决问题的提示</h2> <p><strong>查看连接器工作节点的日志</strong></p> <p>要查看Kafka连接器的错误日志，需要定位到Kafka连接器工作节点的输出。这个位置取决于Kafka连接器是如何启动的，<a href="https://docs.confluent.io/current/installation/installing_cp/index.html?&_ga=2.40515874.1251956415.1553320583-1542045317.1553320583#on-premises-deployments" target="_self" rel="noopener noreferrer">安装Kafka连接器</a>有好几种方法，包括Docker、Confluent CLI、systemd和手动下载的压缩包，然后工作节点的日志分别位于：</p> <ul><li>Docker：<code>docker logs container_name</code>；</li> <li>Confluent CLI：<code>confluent log connect</code>；</li> <li>systemd：日志文件写入<code>/var/log/confluent/kafka-connect</code>；</li> <li>其它：默认情况下，Kafka连接器会将其输出发送到<code>stdout</code>，因此可以在启动Kafka连接器的终端会话中看到。</li></ul> <p><strong>查看Kafka连接器的配置文件</strong></p> <p>要更改Kafka连接器工作节点的配置项（适用于所有运行的连接器），需要相应地做出如下的修改：</p> <ul><li>Docker：配置环境变量，比如在Docker Compose中：</li></ul> <div class="language- line-numbers-mode"><pre class="language-text"><code>CONNECT_KEY_CONVERTER: io.confluent.connect.avro.AvroConverter
CONNECT_KEY_CONVERTER_SCHEMA_REGISTRY_URL: 'http://schema-registry:8081'
CONNECT_VALUE_CONVERTER: io.confluent.connect.avro.AvroConverter
CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: 'http://schema-registry:8081'
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><ul><li>Confluent CLI：使用配置文件<code>/etc/schema-registry/connect-avro-distributed.properties</code>；</li> <li>systemd（deb/rpm）：使用配置文件<code>/etc/kafka/connect-distributed.properties</code>；</li> <li>其它：启动Kafka连接器时，可以指定工作节点的属性文件，例如：</li></ul> <div class="language-bash line-numbers-mode"><pre class="language-bash"><code>$ <span class="token function">cd</span> confluent-5.0.0
$ ./bin/connect-distributed ./etc/kafka/connect-distributed.properties
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p><strong>检查Kafka主题</strong></p> <p>假设遇到了上面提到过的错误，并且想要解决为什么Kafka连接器的接收端无法从主题中读取数据。</p> <p>这时需要检查正在读取的主题的数据，并确认它采用了期望的序列化格式。另外，要记住所有消息都必须采用这种格式，所以不要只是假设因为现在以正确的格式向主题发送消息，所以不会出现问题。Kafka连接器和其它消费者也会读取该主题的已有消息。</p> <p>下面将使用命令行来描述如何进行故障排除，但还有一些其它工具也可以做：</p> <ul><li><a href="https://www.confluent.io/confluent-control-center/" target="_self" rel="noopener noreferrer">Confluent控制中心</a>有通过可视化的方式查看主题内容的功能，包括自动确定序列化格式；</li> <li><a href="https://docs.confluent.io/current/ksql/docs/developer-guide/syntax-reference.html?_ga=2.146003636.1251956415.1553320583-1542045317.1553320583#print" target="_self" rel="noopener noreferrer">KSQL的PRINT命令</a>会将主题的内容打印到控制台，包括自动确定序列化格式；</li> <li><a href="https://docs.confluent.io/current/cli/index.html?_ga=2.84161817.1251956415.1553320583-1542045317.1553320583" target="_self" rel="noopener noreferrer">Confluent CLI</a>工具有<code>consume</code>命令，其可被用于读取字符串和Avro格式的数据。</li></ul> <p><strong>如果认为是字符串/JSON格式数据</strong></p> <p>可以使用控制台工具，包括<code>kafkacat</code>和<code>kafka-console-consumer</code>，以<code>kafkacat</code>为例：</p> <div class="language-bash line-numbers-mode"><pre class="language-bash"><code>$ kafkacat -b localhost:9092 -t users-json-noschema -C -c1
<span class="token punctuation">{</span>
  <span class="token string">&quot;registertime&quot;</span>:1493356576434,<span class="token string">&quot;userid&quot;</span><span class="token keyword">:</span><span class="token string">&quot;User_8&quot;</span>,<span class="token string">&quot;regionid&quot;</span><span class="token keyword">:</span><span class="token string">&quot;Region_2&quot;</span>,<span class="token string">&quot;gender&quot;</span><span class="token keyword">:</span><span class="token string">&quot;MALE&quot;</span><span class="token punctuation">}</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p>使用<code>jq</code>命令，还可以验证和格式化JSON：</p> <div class="language-bash line-numbers-mode"><pre class="language-bash"><code>$ kafkacat -b localhost:9092 -t users-json-noschema -C -c1<span class="token operator">|</span>jq <span class="token string">'.'</span>
<span class="token punctuation">{</span>
  <span class="token string">&quot;registertime&quot;</span><span class="token keyword">:</span> 1493356576434,
  <span class="token string">&quot;userid&quot;</span><span class="token keyword">:</span> <span class="token string">&quot;User_8&quot;</span>,
  <span class="token string">&quot;regionid&quot;</span><span class="token keyword">:</span> <span class="token string">&quot;Region_2&quot;</span>,
  <span class="token string">&quot;gender&quot;</span><span class="token keyword">:</span> <span class="token string">&quot;MALE&quot;</span>
<span class="token punctuation">}</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br></div></div><p>如果你看到了下面这样的乱码字符，其很可能是二进制数据，比如通过Avro或Protobuf格式写入就是这样的：</p> <div class="language-bash line-numbers-mode"><pre class="language-bash"><code>$ kafkacat -b localhost:9092 -t users-avro -C -c1
ڝ���VUser_9Region_MALE
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p><strong>如果认为是Avro格式数据</strong></p> <p>需要使用专为读取和反序列化Avro数据而设计的控制台工具，这里会使用<code>kafka-avro-console-consumer</code>。先要确认指定了正确的模式注册表URL：</p> <div class="language-bash line-numbers-mode"><pre class="language-bash"><code>$ kafka-avro-console-consumer --bootstrap-server localhost:9092 \
                              --property schema.registry.url<span class="token operator">=</span>http://localhost:8081 \
                              --topic users-avro \
                              --from-beginning --max-messages 1
<span class="token punctuation">{</span><span class="token string">&quot;registertime&quot;</span>:1505213905022,<span class="token string">&quot;userid&quot;</span><span class="token keyword">:</span><span class="token string">&quot;User_5&quot;</span>,<span class="token string">&quot;regionid&quot;</span><span class="token keyword">:</span><span class="token string">&quot;Region_4&quot;</span>,<span class="token string">&quot;gender&quot;</span><span class="token keyword">:</span><span class="token string">&quot;FEMALE&quot;</span><span class="token punctuation">}</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><p>和之前一样，如果要对其格式化，可以通过管道输出结果给<code>jq</code>：</p> <div class="language-bash line-numbers-mode"><pre class="language-bash"><code>$ kafka-avro-console-consumer --bootstrap-server localhost:9092 \
                              --property schema.registry.url<span class="token operator">=</span>http://localhost:8081 \
                              --topic users-avro \
                              --from-beginning --max-messages 1 <span class="token operator">|</span> \
                              jq <span class="token string">'.'</span>
<span class="token punctuation">{</span>
  <span class="token string">&quot;registertime&quot;</span><span class="token keyword">:</span> 1505213905022,
  <span class="token string">&quot;userid&quot;</span><span class="token keyword">:</span> <span class="token string">&quot;User_5&quot;</span>,
  <span class="token string">&quot;regionid&quot;</span><span class="token keyword">:</span> <span class="token string">&quot;Region_4&quot;</span>,
  <span class="token string">&quot;gender&quot;</span><span class="token keyword">:</span> <span class="token string">&quot;FEMALE&quot;</span>
<span class="token punctuation">}</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br></div></div><h2 id="内部转换器"><a href="#内部转换器" aria-hidden="true" class="header-anchor">#</a> 内部转换器</h2> <p>当运行在分布式模式时，Kafka连接器使用Kafka本身来存储有关其操作的元数据，包括连接器配置，偏移量等。</p> <p>通过<code>internal.key.converter/internal.value.converter</code>配置项，这些Kafka主题本身可以配置使用不同的转换器。但是这些配置项只供内部使用，实际上<a href="https://cwiki.apache.org/confluence/display/KAFKA/KIP-174+-+Deprecate+and+remove+internal+converter+configs+in+WorkerConfig" target="_self" rel="noopener noreferrer">从Kafka 2.0版本开始就已被弃用</a>。不再需要修改这些，如果还要修改这些配置项，从Kafka的2.0版本开始，将会收到警告。</p> <h2 id="将模式应用于没有模式的消息"><a href="#将模式应用于没有模式的消息" aria-hidden="true" class="header-anchor">#</a> 将模式应用于没有模式的消息</h2> <p>很多时候Kafka连接器会从已经存在模式的地方引入数据，这时只要保留该模式然后使用合适的序列化格式（例如Avro），加上比如模式注册表等提供的兼容性保证，该数据的所有下游用户就都可以从可用的模式中受益。但是如果没有明确的模式呢？</p> <p>可能正使用<a href="https://docs.confluent.io/current/connect/filestream_connector.html?_ga=2.142315826.1251956415.1553320583-1542045317.1553320583" target="_self" rel="noopener noreferrer">FileSourceConnector</a>从纯文本文件中读取数据（不建议用于生产，但通常用于PoC），或者可能正在使用<a href="https://github.com/llofberg/kafka-connect-rest" target="_self" rel="noopener noreferrer">REST连接器</a>从REST端点提取数据。由于这两者以及其它的都没有固有的模式，因此需要进行声明。</p> <p>有时可能只是想从源端读取字节然后将它们写入一个主题上，但大多数情况下需要做正确的事情并应用模式以便数据可以正确地处理。作为数据提取的一部分处理一次，而不是将问题推送到每个消费者（可能是多个），这是一个更好的做法。</p> <p>可以编写自己的Kafka流式应用以将模式应用于Kafka主题中的数据，但也可以使用KSQL。<a href="https://www.confluent.io/blog/data-wrangling-apache-kafka-ksql" target="_self" rel="noopener noreferrer">这篇文章</a>展示了如何对从REST端点提取的JSON数据执行此操作。下面会看一下将模式应用于某些CSV数据的简单示例，显然是可以做到的。</p> <p>假设有一个名为<code>testdata-csv</code>的Kafka主题，其中有一些CSV数据，大致如下：</p> <div class="language-bash line-numbers-mode"><pre class="language-bash"><code>$ kafkacat -b localhost:9092 -t testdata-csv -C
1,Rick Astley,Never Gonna Give You Up
2,Johnny Cash,Ring of Fire
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p>通过观察，可以猜测它有三个字段，可能为：</p> <ul><li>ID；</li> <li>艺术家；</li> <li>歌曲。</li></ul> <p>如果将数据保留在这样的主题中，那么任何想要使用该数据的应用程序（可能是Kafka连接器接收端、定制的Kafka应用或者其它），都需要每次猜测这个模式。或者更糟糕的是，每个消费端应用的开发者都需要不断向数据提供方确认模式及其任何变更。正如Kafka解耦系统一样，这种模式依赖性迫使团队之间存在硬性耦合，这并不是一件好事。</p> <p>因此要做的只是使用KSQL将模式应用于数据，并填充一个新的派生主题，其中保存模式。在KSQL中，可以像下面这样查看主题数据：</p> <div class="language- line-numbers-mode"><pre class="language-text"><code>ksql&gt; PRINT 'testdata-csv' FROM BEGINNING;
Format:STRING
11/6/18 2:41:23 PM UTC , NULL , 1,Rick Astley,Never Gonna Give You Up
11/6/18 2:41:23 PM UTC , NULL , 2,Johnny Cash,Ring of Fire
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><p>这里的前两个字段（<code>11/6/18 2:41:23 PM UTC</code>和<code>NULL</code>）分别是Kafka消息的时间戳和键，而其余字段来自CSV文件。下面用KSQL注册这个主题并声明模式：</p> <div class="language- line-numbers-mode"><pre class="language-text"><code>ksql&gt; CREATE STREAM TESTDATA_CSV (ID INT, ARTIST VARCHAR, SONG VARCHAR) \
WITH (KAFKA_TOPIC='testdata-csv', VALUE_FORMAT='DELIMITED');

Message
----------------
Stream created
----------------
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br></div></div><p>通过KSQL可以查看现在有一个数据流模式：</p> <div class="language- line-numbers-mode"><pre class="language-text"><code>ksql&gt; DESCRIBE TESTDATA_CSV;

Name                 : TESTDATA_CSV
 Field   | Type
-------------------------------------
 ROWTIME | BIGINT (system)
 ROWKEY  | VARCHAR(STRING) (system)
 ID      | INTEGER
 ARTIST  | VARCHAR(STRING)
 SONG    | VARCHAR(STRING)
-------------------------------------
For runtime statistics and query details run: DESCRIBE EXTENDED &lt;Stream,Table&gt;;
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br></div></div><p>通过查询KSQL流确认数据是否符合预期。注意对于已有的Kafka主题，此时只是作为Kafka的消费者，尚未更改或复制任何数据。</p> <div class="language- line-numbers-mode"><pre class="language-text"><code>ksql&gt; SET 'auto.offset.reset' = 'earliest';
Successfully changed local property 'auto.offset.reset' from 'null' to 'earliest'
ksql&gt; SELECT ID, ARTIST, SONG FROM TESTDATA_CSV;
1 | Rick Astley | Never Gonna Give You Up
2 | Johnny Cash | Ring of Fire
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><p>最后，创建一个新的Kafka主题，由具有模式的重新序列化的数据填充。KSQL查询是连续的，因此除了将任何已有的数据从源端主题发送到目标端主题之外，KSQL还将向主题发送任何未来的数据。</p> <div class="language- line-numbers-mode"><pre class="language-text"><code>ksql&gt; CREATE STREAM TESTDATA WITH (VALUE_FORMAT='AVRO') AS SELECT * FROM TESTDATA_CSV;

Message
----------------------------
Stream created and running
----------------------------
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br></div></div><p>这时使用Avro格式的控制台消费者对数据进行验证：</p> <div class="language- line-numbers-mode"><pre class="language-text"><code>$ kafka-avro-console-consumer --bootstrap-server localhost:9092 \
                                --property schema.registry.url=http://localhost:8081 \
                                --topic TESTDATA \
                                --from-beginning | \
                                jq '.'
{
  &quot;ID&quot;: {
    &quot;int&quot;: 1
},
  &quot;ARTIST&quot;: {
    &quot;string&quot;: &quot;Rick Astley&quot;
},
  &quot;SONG&quot;: {
    &quot;string&quot;: &quot;Never Gonna Give You Up&quot;
  }
}
[…]
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br></div></div><p>甚至可以在模式注册表中查看已经注册的模式：</p> <div class="language- line-numbers-mode"><pre class="language-text"><code>$ curl -s http://localhost:8081/subjects/TESTDATA-value/versions/latest|jq '.schema|fromjson'
{
  &quot;type&quot;: &quot;record&quot;,
  &quot;name&quot;: &quot;KsqlDataSourceSchema&quot;,
  &quot;namespace&quot;: &quot;io.confluent.ksql.avro_schemas&quot;,
  &quot;fields&quot;: [
    {
      &quot;name&quot;: &quot;ID&quot;,
      &quot;type&quot;: [
        &quot;null&quot;,
        &quot;int&quot;
      ],
      &quot;default&quot;: null
    },
    {
      &quot;name&quot;: &quot;ARTIST&quot;,
      &quot;type&quot;: [
        &quot;null&quot;,
        &quot;string&quot;
      ],
      &quot;default&quot;: null
    },
    {
      &quot;name&quot;: &quot;SONG&quot;,
      &quot;type&quot;: [
        &quot;null&quot;,
        &quot;string&quot;
      ],
      &quot;default&quot;: null
    }
  ]
}
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br><span class="line-number">27</span><br><span class="line-number">28</span><br><span class="line-number">29</span><br><span class="line-number">30</span><br><span class="line-number">31</span><br><span class="line-number">32</span><br></div></div><p>任何写入原始主题（<code>testdata-csv</code>）的新消息都由KSQL自动处理，并写入Avro格式的名为<code>TESTDATA</code>的新主题。现在，任何想要使用此数据的应用或团队都可以简单地处理<code>TESTDATA</code>主题，并利用声明模式的Avro序列化数据。还可以使用此技术更改主题中的分区数，分区键和复制因子。</p> <h2 id="结论"><a href="#结论" aria-hidden="true" class="header-anchor">#</a> 结论</h2> <p>Kafka连接器是一个非常简单但功能强大的工具，可用于将其它系统与Kafka集成，最常见的误解是Kafka连接器提供的转换器。之前已经介绍过Kafka消息只是键/值对，了解应该使用哪个序列化机制然后在Kafka连接器中对其进行标准化非常重要。</p></div> <div class="page-edit"><!----> <div class="last-updated"><span class="prefix">最后更新时间：: </span> <span class="time">3/27/2019, 1:11:07 PM</span></div></div> <div class="page-nav"><p class="inner"><span class="prev">
        ←
        <a href="/confluent/" class="prev router-link-active">
          Kafka连接器深度解读之JDBC源连接器
        </a></span> <span class="next"><a href="/confluent/Kafka-ErrorHandlingDeadLetterQueues.html">
          Kafka连接器深度解读之错误处理和死信队列
        </a>
        →
      </span></p></div> </div> <!----></div></div>
    <script src="/assets/js/app.b4afcc0e.js" defer></script><script src="/assets/js/8.447d2fe0.js" defer></script>
  </body>
</html>
