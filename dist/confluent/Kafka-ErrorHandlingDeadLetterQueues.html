<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Kafka连接器深度解读之错误处理和死信队列 | Apache Ignite中文网</title>
    <meta name="description" content="轻量级分布式内存HTAP数据库及计算平台GridGain中国大陆地区独家合作伙伴">
    <meta http-equiv="cache-control" content="no-store,no-cache,must-revalidate">
  <meta http-equiv="expires" content="0">
  <link rel="shortcut icon" type="image/x-icon" href="https://ignite.apache.org/favicon.ico">
  <script src="https://hm.baidu.com/hm.js?03f40be28ff9a31fd798fd6b9dac0946"></script>
    
    <link rel="preload" href="/assets/css/0.styles.8148ff4f.css" as="style"><link rel="preload" href="/assets/js/app.9949b77c.js" as="script"><link rel="preload" href="/assets/js/2.6817020a.js" as="script"><link rel="preload" href="/assets/js/13.a93a44d2.js" as="script"><link rel="prefetch" href="/assets/js/10.37c58d01.js"><link rel="prefetch" href="/assets/js/100.b7a3220d.js"><link rel="prefetch" href="/assets/js/101.90531654.js"><link rel="prefetch" href="/assets/js/102.9d05bcaa.js"><link rel="prefetch" href="/assets/js/103.39e33f6a.js"><link rel="prefetch" href="/assets/js/104.3be0515b.js"><link rel="prefetch" href="/assets/js/105.adc71e82.js"><link rel="prefetch" href="/assets/js/106.caa3b1cc.js"><link rel="prefetch" href="/assets/js/107.fa165bf9.js"><link rel="prefetch" href="/assets/js/108.6282924a.js"><link rel="prefetch" href="/assets/js/109.142b63d4.js"><link rel="prefetch" href="/assets/js/11.5dbca4cc.js"><link rel="prefetch" href="/assets/js/110.d79ce6a0.js"><link rel="prefetch" href="/assets/js/111.25423ae2.js"><link rel="prefetch" href="/assets/js/112.0ba3920a.js"><link rel="prefetch" href="/assets/js/113.85053f80.js"><link rel="prefetch" href="/assets/js/114.ae7ba213.js"><link rel="prefetch" href="/assets/js/115.2c2aa3d4.js"><link rel="prefetch" href="/assets/js/116.ef1ea7db.js"><link rel="prefetch" href="/assets/js/117.473b5f3f.js"><link rel="prefetch" href="/assets/js/118.3c24774e.js"><link rel="prefetch" href="/assets/js/119.3a6e9ef9.js"><link rel="prefetch" href="/assets/js/12.e4b88198.js"><link rel="prefetch" href="/assets/js/120.f809c0ba.js"><link rel="prefetch" href="/assets/js/121.41dd2aa5.js"><link rel="prefetch" href="/assets/js/122.8c5cd493.js"><link rel="prefetch" href="/assets/js/123.d36e4ae6.js"><link rel="prefetch" href="/assets/js/124.83e36e04.js"><link rel="prefetch" href="/assets/js/125.eec64b8d.js"><link rel="prefetch" href="/assets/js/14.0ded4399.js"><link rel="prefetch" href="/assets/js/15.69dba176.js"><link rel="prefetch" href="/assets/js/16.fe88c450.js"><link rel="prefetch" href="/assets/js/17.35fceae5.js"><link rel="prefetch" href="/assets/js/18.0bb4b326.js"><link rel="prefetch" href="/assets/js/19.bea31ec9.js"><link rel="prefetch" href="/assets/js/20.699bc94a.js"><link rel="prefetch" href="/assets/js/21.ef5fa8a4.js"><link rel="prefetch" href="/assets/js/22.74955344.js"><link rel="prefetch" href="/assets/js/23.808e0285.js"><link rel="prefetch" href="/assets/js/24.2a0a21c5.js"><link rel="prefetch" href="/assets/js/25.f5eac7d3.js"><link rel="prefetch" href="/assets/js/26.cf742e25.js"><link rel="prefetch" href="/assets/js/27.cbd0d5df.js"><link rel="prefetch" href="/assets/js/28.43524cd4.js"><link rel="prefetch" href="/assets/js/29.b2841c77.js"><link rel="prefetch" href="/assets/js/3.fdd31f3c.js"><link rel="prefetch" href="/assets/js/30.5351be28.js"><link rel="prefetch" href="/assets/js/31.1dedf05b.js"><link rel="prefetch" href="/assets/js/32.23c2d947.js"><link rel="prefetch" href="/assets/js/33.a46da537.js"><link rel="prefetch" href="/assets/js/34.ad9a770c.js"><link rel="prefetch" href="/assets/js/35.7a2fc720.js"><link rel="prefetch" href="/assets/js/36.ea82918e.js"><link rel="prefetch" href="/assets/js/37.e4bfce82.js"><link rel="prefetch" href="/assets/js/38.61fdfd4c.js"><link rel="prefetch" href="/assets/js/39.5083418e.js"><link rel="prefetch" href="/assets/js/4.9fe5e945.js"><link rel="prefetch" href="/assets/js/40.d949749b.js"><link rel="prefetch" href="/assets/js/41.d64cbd1e.js"><link rel="prefetch" href="/assets/js/42.0a8244cc.js"><link rel="prefetch" href="/assets/js/43.ff5fab23.js"><link rel="prefetch" href="/assets/js/44.4e33238d.js"><link rel="prefetch" href="/assets/js/45.a97fa465.js"><link rel="prefetch" href="/assets/js/46.752c1a39.js"><link rel="prefetch" href="/assets/js/47.c2bd2a6d.js"><link rel="prefetch" href="/assets/js/48.03450bd2.js"><link rel="prefetch" href="/assets/js/49.1686ade1.js"><link rel="prefetch" href="/assets/js/5.ace6695b.js"><link rel="prefetch" href="/assets/js/50.4c9ac129.js"><link rel="prefetch" href="/assets/js/51.db369560.js"><link rel="prefetch" href="/assets/js/52.d4842a10.js"><link rel="prefetch" href="/assets/js/53.0fc1b4a2.js"><link rel="prefetch" href="/assets/js/54.0aedadc9.js"><link rel="prefetch" href="/assets/js/55.80bdadb5.js"><link rel="prefetch" href="/assets/js/56.2bca4ad0.js"><link rel="prefetch" href="/assets/js/57.68669ba7.js"><link rel="prefetch" href="/assets/js/58.79bad7cf.js"><link rel="prefetch" href="/assets/js/59.4abd9693.js"><link rel="prefetch" href="/assets/js/6.c53fbeaa.js"><link rel="prefetch" href="/assets/js/60.b4af12f7.js"><link rel="prefetch" href="/assets/js/61.2dadf50a.js"><link rel="prefetch" href="/assets/js/62.82b0f2ea.js"><link rel="prefetch" href="/assets/js/63.652298a2.js"><link rel="prefetch" href="/assets/js/64.43cd25b6.js"><link rel="prefetch" href="/assets/js/65.cf45a0f9.js"><link rel="prefetch" href="/assets/js/66.935487c2.js"><link rel="prefetch" href="/assets/js/67.2b9016e0.js"><link rel="prefetch" href="/assets/js/68.ebf3db25.js"><link rel="prefetch" href="/assets/js/69.1697c33b.js"><link rel="prefetch" href="/assets/js/7.e4e81c9c.js"><link rel="prefetch" href="/assets/js/70.7a64bd11.js"><link rel="prefetch" href="/assets/js/71.d75a27bc.js"><link rel="prefetch" href="/assets/js/72.a898ab43.js"><link rel="prefetch" href="/assets/js/73.66deb744.js"><link rel="prefetch" href="/assets/js/74.bd9fa323.js"><link rel="prefetch" href="/assets/js/75.d1dceae5.js"><link rel="prefetch" href="/assets/js/76.ac1e3dfd.js"><link rel="prefetch" href="/assets/js/77.7f2d1cb8.js"><link rel="prefetch" href="/assets/js/78.8174512d.js"><link rel="prefetch" href="/assets/js/79.380c286d.js"><link rel="prefetch" href="/assets/js/8.cfdd68fa.js"><link rel="prefetch" href="/assets/js/80.53d74284.js"><link rel="prefetch" href="/assets/js/81.ef11a7bc.js"><link rel="prefetch" href="/assets/js/82.0455141e.js"><link rel="prefetch" href="/assets/js/83.1237aef5.js"><link rel="prefetch" href="/assets/js/84.850d5ad5.js"><link rel="prefetch" href="/assets/js/85.fa8b6296.js"><link rel="prefetch" href="/assets/js/86.633d31c7.js"><link rel="prefetch" href="/assets/js/87.573e0d47.js"><link rel="prefetch" href="/assets/js/88.7e2b1166.js"><link rel="prefetch" href="/assets/js/89.a8c77dfa.js"><link rel="prefetch" href="/assets/js/9.35d6546a.js"><link rel="prefetch" href="/assets/js/90.722bd7d3.js"><link rel="prefetch" href="/assets/js/91.09d63920.js"><link rel="prefetch" href="/assets/js/92.56189b71.js"><link rel="prefetch" href="/assets/js/93.1800484e.js"><link rel="prefetch" href="/assets/js/94.c859a714.js"><link rel="prefetch" href="/assets/js/95.7755f804.js"><link rel="prefetch" href="/assets/js/96.00e8ed34.js"><link rel="prefetch" href="/assets/js/97.c9f25a09.js"><link rel="prefetch" href="/assets/js/98.0d379cc7.js"><link rel="prefetch" href="/assets/js/99.2dd39086.js">
    <link rel="stylesheet" href="/assets/css/0.styles.8148ff4f.css">
  </head>
  <body>
    <div id="app" data-server-rendered="true"><div class="theme-container"><header class="navbar"><div class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/" class="home-link router-link-active"><!----> <span class="site-name">Apache Ignite中文网</span></a> <div class="links"><!----> <nav class="nav-links can-hide"><div class="nav-item"><a href="/" class="nav-link">首页</a></div><div class="nav-item"><a href="/doc/java/" class="nav-link">Java</a></div><div class="nav-item"><a href="/doc/cpp/" class="nav-link">C++</a></div><div class="nav-item"><a href="/doc/net/" class="nav-link">C#/.NET</a></div><div class="nav-item"><a href="/doc/sql/" class="nav-link">SQL</a></div><div class="nav-item"><a href="/doc/integration/" class="nav-link">集成</a></div><div class="nav-item"><a href="/doc/spark/" class="nav-link">Ignite&amp;Spark</a></div><div class="nav-item"><a href="/doc/tools/" class="nav-link">工具</a></div><div class="nav-item"><a href="https://my.oschina.net/liyuj" target="_blank" rel="noopener noreferrer" class="nav-link external">
  博客
  <svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></div><div class="nav-item"><a href="/confluent/" class="nav-link router-link-active">Confluent平台</a></div><div class="nav-item"><a href="https://www.zybuluo.com/liyuj/note/230739" target="_blank" rel="noopener noreferrer" class="nav-link external">
  历史版本
  <svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="Select language" class="dropdown-title"><span class="title">最新版本</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/confluent/Kafka-ErrorHandlingDeadLetterQueues.html" class="nav-link router-link-exact-active router-link-active">2.7.0</a></li><li class="dropdown-item"><!----> <a href="/doc/2.6.0/" class="nav-link">2.6.0</a></li></ul></div></div> <!----></nav></div></header> <div class="sidebar-mask"></div> <aside class="sidebar"><nav class="nav-links"><div class="nav-item"><a href="/" class="nav-link">首页</a></div><div class="nav-item"><a href="/doc/java/" class="nav-link">Java</a></div><div class="nav-item"><a href="/doc/cpp/" class="nav-link">C++</a></div><div class="nav-item"><a href="/doc/net/" class="nav-link">C#/.NET</a></div><div class="nav-item"><a href="/doc/sql/" class="nav-link">SQL</a></div><div class="nav-item"><a href="/doc/integration/" class="nav-link">集成</a></div><div class="nav-item"><a href="/doc/spark/" class="nav-link">Ignite&amp;Spark</a></div><div class="nav-item"><a href="/doc/tools/" class="nav-link">工具</a></div><div class="nav-item"><a href="https://my.oschina.net/liyuj" target="_blank" rel="noopener noreferrer" class="nav-link external">
  博客
  <svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></div><div class="nav-item"><a href="/confluent/" class="nav-link router-link-active">Confluent平台</a></div><div class="nav-item"><a href="https://www.zybuluo.com/liyuj/note/230739" target="_blank" rel="noopener noreferrer" class="nav-link external">
  历史版本
  <svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="Select language" class="dropdown-title"><span class="title">最新版本</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/confluent/Kafka-ErrorHandlingDeadLetterQueues.html" class="nav-link router-link-exact-active router-link-active">2.7.0</a></li><li class="dropdown-item"><!----> <a href="/doc/2.6.0/" class="nav-link">2.6.0</a></li></ul></div></div> <!----></nav>  <ul class="sidebar-links"><li><a href="/confluent/" class="sidebar-link">Kafka连接器深度解读之JDBC源连接器</a></li><li><a href="/confluent/Kafka-ConvertersSerialization.html" class="sidebar-link">Kafka连接器深度解读之转换器和序列化</a></li><li><a href="/confluent/Kafka-ErrorHandlingDeadLetterQueues.html" class="active sidebar-link">Kafka连接器深度解读之错误处理和死信队列</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/confluent/Kafka-ErrorHandlingDeadLetterQueues.html#失败后立即停止" class="sidebar-link">失败后立即停止</a></li><li class="sidebar-sub-header"><a href="/confluent/Kafka-ErrorHandlingDeadLetterQueues.html#静默忽略无效的消息" class="sidebar-link">静默忽略无效的消息</a></li><li class="sidebar-sub-header"><a href="/confluent/Kafka-ErrorHandlingDeadLetterQueues.html#是否可以感知数据的丢失？" class="sidebar-link">是否可以感知数据的丢失？</a></li><li class="sidebar-sub-header"><a href="/confluent/Kafka-ErrorHandlingDeadLetterQueues.html#将消息路由到死信队列" class="sidebar-link">将消息路由到死信队列</a></li><li class="sidebar-sub-header"><a href="/confluent/Kafka-ErrorHandlingDeadLetterQueues.html#记录消息的失败原因：消息头" class="sidebar-link">记录消息的失败原因：消息头</a></li><li class="sidebar-sub-header"><a href="/confluent/Kafka-ErrorHandlingDeadLetterQueues.html#记录消息的失败原因：日志" class="sidebar-link">记录消息的失败原因：日志</a></li><li class="sidebar-sub-header"><a href="/confluent/Kafka-ErrorHandlingDeadLetterQueues.html#处理死信队列的消息" class="sidebar-link">处理死信队列的消息</a></li><li class="sidebar-sub-header"><a href="/confluent/Kafka-ErrorHandlingDeadLetterQueues.html#通过ksql监控死信队列" class="sidebar-link">通过KSQL监控死信队列</a></li><li class="sidebar-sub-header"><a href="/confluent/Kafka-ErrorHandlingDeadLetterQueues.html#kafka连接器哪里没有提供错误处理？" class="sidebar-link">Kafka连接器哪里没有提供错误处理？</a></li><li class="sidebar-sub-header"><a href="/confluent/Kafka-ErrorHandlingDeadLetterQueues.html#错误处理配置流程" class="sidebar-link">错误处理配置流程</a></li><li class="sidebar-sub-header"><a href="/confluent/Kafka-ErrorHandlingDeadLetterQueues.html#总结" class="sidebar-link">总结</a></li></ul></li><li><a href="/confluent/Kafka-ConnectImprovementsIn-2-3.html" class="sidebar-link">Kafka连接器之在2.3版本中的改进</a></li></ul> </aside> <main class="page"> <div class="theme-default-content content__default"><h1 id="kafka连接器深度解读之错误处理和死信队列"><a href="#kafka连接器深度解读之错误处理和死信队列" aria-hidden="true" class="header-anchor">#</a> Kafka连接器深度解读之错误处理和死信队列</h1> <p>Kafka连接器是Kafka的一部分，是在Kafka和其它技术之间构建流式管道的一个强有力的框架。它可用于将数据从多个地方（包括数据库、消息队列和文本文件）流式注入到Kafka，以及从Kafka将数据流式传输到目标端（如文档存储、NoSQL、数据库、对象存储等）中。</p> <p>现实世界并不完美，出错是难免的，因此在出错时Kafka的管道能尽可能优雅地处理是最好的。一个常见的场景是获取与特定序列化格式不匹配的主题的消息（比如预期为Avro时实际为JSON，反之亦然）。自从Kafka 2.0版本发布以来，Kafka连接器包含了错误处理选项，即将消息路由到<em>死信队列</em>的功能，这是构建数据管道的常用技术。</p> <p>在本文中将介绍几种处理问题的常见模式，并说明如何实现。</p> <h2 id="失败后立即停止"><a href="#失败后立即停止" aria-hidden="true" class="header-anchor">#</a> 失败后立即停止</h2> <p>有时可能希望在发生错误时立即停止处理，可能遇到质量差的数据是由于上游的原因导致的，必须由上游来解决，继续尝试处理其它的消息已经没有意义。</p> <p><img src="https://www.confluent.io/wp-content/uploads/Source_Topic_Messages_Kafka_Connect_Sink_Messages-e1552329568691.png" alt=""></p> <p>这是Kafka连接器的默认行为，也可以使用下面的配置项显式地指定：</p> <div class="language-properties line-numbers-mode"><pre class="language-properties"><code><span class="token attr-name">errors.tolerance</span> <span class="token punctuation">=</span> <span class="token attr-value">none</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>在本示例中，该连接器配置为从主题中读取JSON格式数据，然后将其写入纯文本文件。注意这里为了演示使用的是<code>FileStreamSinkConnector</code>连接器，不建议在生产中使用。</p> <div class="language- line-numbers-mode"><pre class="language-text"><code>curl -X POST http://localhost:8083/connectors -H &quot;Content-Type: application/json&quot; -d '{
        &quot;name&quot;: &quot;file_sink_01&quot;,
        &quot;config&quot;: {
                &quot;connector.class&quot;: &quot;org.apache.kafka.connect.file.FileStreamSinkConnector&quot;,
                &quot;topics&quot;:&quot;test_topic_json&quot;,
                &quot;value.converter&quot;:&quot;org.apache.kafka.connect.json.JsonConverter&quot;,
                &quot;value.converter.schemas.enable&quot;: false,
                &quot;key.converter&quot;:&quot;org.apache.kafka.connect.json.JsonConverter&quot;,
                &quot;key.converter.schemas.enable&quot;: false,
                &quot;file&quot;:&quot;/data/file_sink_01.txt&quot;
                }
        }'
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br></div></div><p>主题中的某些JSON格式消息是无效的，连接器会立即终止，进入以下的<code>FAILED</code>状态：</p> <div class="language-bash line-numbers-mode"><pre class="language-bash"><code>$ <span class="token function">curl</span> -s <span class="token string">&quot;http://localhost:8083/connectors/file_sink_01/status&quot;</span><span class="token operator">|</span> <span class="token punctuation">\</span>
    jq -c -M <span class="token string">'[.name,.tasks[].state]'</span>
<span class="token punctuation">[</span><span class="token string">&quot;file_sink_01&quot;</span>,<span class="token string">&quot;FAILED&quot;</span><span class="token punctuation">]</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p>查看Kafka连接器工作节点的日志，可以看到错误已经记录并且任务已经终止：</p> <div class="language- line-numbers-mode"><pre class="language-text"><code>org.apache.kafka.connect.errors.ConnectException: Tolerance exceeded in error handler
 at org.apache.kafka.connect.runtime.errors.RetryWithToleranceOperator.execAndHandleError(RetryWithToleranceOperator.java:178)
…
Caused by: org.apache.kafka.connect.errors.DataException: Converting byte[] to Kafka Connect data failed due to serialization error:
 at org.apache.kafka.connect.json.JsonConverter.toConnectData(JsonConverter.java:334)
…
Caused by: org.apache.kafka.common.errors.SerializationException: com.fasterxml.jackson.core.JsonParseException: Unexpected character ('b' (code 98)): was expecting double-quote to start field name
 at [Source: (byte[])&quot;{brokenjson-:&quot;bar 1&quot;}&quot;; line: 1, column: 3]
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br></div></div><p>要修复管道，需要解决源主题上的消息问题。除非事先指定，Kafka连接器是不会简单地“跳过”无效消息的。如果是配置错误（例如指定了错误的序列化转换器），那最好了，改正之后重新启动连接器即可。不过如果确实是该主题的无效消息，那么需要找到一种方式，即不要阻止所有其它有效消息的处理。</p> <h2 id="静默忽略无效的消息"><a href="#静默忽略无效的消息" aria-hidden="true" class="header-anchor">#</a> 静默忽略无效的消息</h2> <p>如果只是希望处理一直持续下去：</p> <div class="language-properties line-numbers-mode"><pre class="language-properties"><code><span class="token attr-name">errors.tolerance</span> <span class="token punctuation">=</span> <span class="token attr-value">all</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p><img src="https://www.confluent.io/wp-content/uploads/Source_Topic_Messages_Kafka_Connect_Sink_Messages_v2-e1552330256955.png" alt=""></p> <p>在实际中大概如下：</p> <div class="language- line-numbers-mode"><pre class="language-text"><code>curl -X POST http://localhost:8083/connectors -H &quot;Content-Type: application/json&quot; -d '{
        &quot;name&quot;: &quot;file_sink_05&quot;,
        &quot;config&quot;: {
                &quot;connector.class&quot;: &quot;org.apache.kafka.connect.file.FileStreamSinkConnector&quot;,
                &quot;topics&quot;:&quot;test_topic_json&quot;,
                &quot;value.converter&quot;:&quot;org.apache.kafka.connect.json.JsonConverter&quot;,
                &quot;value.converter.schemas.enable&quot;: false,
                &quot;key.converter&quot;:&quot;org.apache.kafka.connect.json.JsonConverter&quot;,
                &quot;key.converter.schemas.enable&quot;: false,
                &quot;file&quot;:&quot;/data/file_sink_05.txt&quot;,
                &quot;errors.tolerance&quot;: &quot;all&quot;
                }
        }'
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br></div></div><p>启动连接器之后（还是原来的源主题，其中既有有效的，也有无效的消息），就可以持续地运行：</p> <div class="language- line-numbers-mode"><pre class="language-text"><code>$ curl -s &quot;http://localhost:8083/connectors/file_sink_05/status&quot;| \
    jq -c -M '[.name,.tasks[].state]'
[&quot;file_sink_05&quot;,&quot;RUNNING&quot;]
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p>这时即使连接器读取的源主题上有无效的消息，也不会有错误写入Kafka连接器工作节点的输出，而有效的消息会按照预期写入输出文件：</p> <div class="language- line-numbers-mode"><pre class="language-text"><code>$ head data/file_sink_05.txt
{foo=bar 1}
{foo=bar 2}
{foo=bar 3}
…
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><h2 id="是否可以感知数据的丢失？"><a href="#是否可以感知数据的丢失？" aria-hidden="true" class="header-anchor">#</a> 是否可以感知数据的丢失？</h2> <p>配置了<code>errors.tolerance = all</code>之后，Kafka连接器就会忽略掉无效的消息，并且默认也不会记录被丢弃的消息。如果确认配置<code>errors.tolerance = all</code>，那么就需要仔细考虑是否以及如何知道实际上发生的消息丢失。在实践中这意味着基于可用指标的监控/报警，和/或失败消息的记录。</p> <p>确定是否有消息被丢弃的最简单方法，是将源主题上的消息数与写入目标端的数量进行对比：</p> <div class="language- line-numbers-mode"><pre class="language-text"><code>$ kafkacat -b localhost:9092 -t test_topic_json -o beginning -C -e -q -X enable.partition.eof=true | wc -l
     150

$ wc -l data/file_sink_05.txt
     100 data/file_sink_05.txt
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><p>这个做法虽然不是很优雅，但是确实能看出发生了消息的丢失，并且因为日志中没有记录，所以用户仍然对此一无所知。</p> <p>一个更加可靠的办法是，使用<a href="https://kafka.apache.org/documentation/#connect_monitoring" target="_self" rel="noopener noreferrer">JMX指标</a>来主动监控和报警错误消息率：</p> <p><img src="https://www.confluent.io/wp-content/uploads/Kafka_Connect_Totals-e1552339993226.png" alt=""></p> <p>这时可以看到发生了错误，但是并不知道那些消息发生了错误，不过这是用户想要的。其实即使之后这些被丢弃的消息被写入了<code>/dev/null</code>，实际上也是可以知道的，这也正是死信队列概念出现的点。</p> <h2 id="将消息路由到死信队列"><a href="#将消息路由到死信队列" aria-hidden="true" class="header-anchor">#</a> 将消息路由到死信队列</h2> <p>Kafka连接器可以配置为将无法处理的消息（例如上面提到的反序列化错误）发送到一个单独的Kafka主题，即死信队列。有效消息会正常处理，管道也会继续运行。然后可以从死信队列中检查无效消息，并根据需要忽略或修复并重新处理。</p> <p><img src="https://www.confluent.io/wp-content/uploads/DLQ_Source_Topic_Messages_Kafka_Connect_Sink_Messages-e1552339900964.png" alt=""></p> <p>进行如下的配置可以启用死信队列：</p> <div class="language-properties line-numbers-mode"><pre class="language-properties"><code><span class="token attr-name">errors.tolerance</span> <span class="token punctuation">=</span> <span class="token attr-value">all</span>
<span class="token attr-name">errors.deadletterqueue.topic.name</span> <span class="token punctuation">=</span><span class="token attr-value"> </span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>如果运行于单节点Kafka集群，还需要配置<code>errors.deadletterqueue.topic.replication.factor = 1</code>，其默认值为3。</p> <p>具有此配置的连接器配置示例大致如下：</p> <div class="language- line-numbers-mode"><pre class="language-text"><code>curl -X POST http://localhost:8083/connectors -H &quot;Content-Type: application/json&quot; -d '{
        &quot;name&quot;: &quot;file_sink_02&quot;,
        &quot;config&quot;: {
                &quot;connector.class&quot;: &quot;org.apache.kafka.connect.file.FileStreamSinkConnector&quot;,
                &quot;topics&quot;:&quot;test_topic_json&quot;,
                &quot;value.converter&quot;:&quot;org.apache.kafka.connect.json.JsonConverter&quot;,
                &quot;value.converter.schemas.enable&quot;: false,
                &quot;key.converter&quot;:&quot;org.apache.kafka.connect.json.JsonConverter&quot;,
                &quot;key.converter.schemas.enable&quot;: false,
                &quot;file&quot;: &quot;/data/file_sink_02.txt&quot;,
                &quot;errors.tolerance&quot;: &quot;all&quot;,
                &quot;errors.deadletterqueue.topic.name&quot;:&quot;dlq_file_sink_02&quot;,
                &quot;errors.deadletterqueue.topic.replication.factor&quot;: 1
                }
        }'
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br></div></div><p>使用和之前相同的源主题，然后处理混合有有效和无效的JSON数据，会看到新的连接器可以稳定运行：</p> <div class="language-bash line-numbers-mode"><pre class="language-bash"><code>$ <span class="token function">curl</span> -s <span class="token string">&quot;http://localhost:8083/connectors/file_sink_02/status&quot;</span><span class="token operator">|</span> <span class="token punctuation">\</span>
    jq -c -M <span class="token string">'[.name,.tasks[].state]'</span>
<span class="token punctuation">[</span><span class="token string">&quot;file_sink_02&quot;</span>,<span class="token string">&quot;RUNNING&quot;</span><span class="token punctuation">]</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p>源主题中的有效记录将写入目标文件：</p> <div class="language-bash line-numbers-mode"><pre class="language-bash"><code>$ <span class="token function">head</span> data/file_sink_02.txt
<span class="token punctuation">{</span>foo<span class="token operator">=</span>bar <span class="token number">1</span><span class="token punctuation">}</span>
<span class="token punctuation">{</span>foo<span class="token operator">=</span>bar <span class="token number">2</span><span class="token punctuation">}</span>
<span class="token punctuation">{</span>foo<span class="token operator">=</span>bar <span class="token number">3</span><span class="token punctuation">}</span>
<span class="token punctuation">[</span>…<span class="token punctuation">]</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><p>这样管道可以继续正常运行，并且还有了死信队列主题中的数据，这可以从指标数据中看出：</p> <p><img src="https://www.confluent.io/wp-content/uploads/Kafka_Connect_Graph-e1552408650942.png" alt=""></p> <p>检查主题本身也可以看出来：</p> <div class="language- line-numbers-mode"><pre class="language-text"><code>ksql&gt; LIST TOPICS;

 Kafka Topic            | Registered | Partitions | Partition Replicas | Consumers | ConsumerGroups
---------------------------------------------------------------------------------------------------
 dlq_file_sink_02       | false      | 1          | 1                  | 0         | 0
 test_topic_json        | false      | 1          | 1                  | 1         | 1
---------------------------------------------------------------------------------------------------

ksql&gt; PRINT 'dlq_file_sink_02' FROM BEGINNING;
Format:STRING
1/24/19 5:16:03 PM UTC , NULL , {foo:&quot;bar 1&quot;}
1/24/19 5:16:03 PM UTC , NULL , {foo:&quot;bar 2&quot;}
1/24/19 5:16:03 PM UTC , NULL , {foo:&quot;bar 3&quot;}
…
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br></div></div><p>从输出中可以看出，消息的时间戳为（<code>1/24/19 5:16:03 PM UTC</code>），键为（<code>NULL</code>），然后为值。这时可以看到值是无效的JSON格式<code>{foo:&quot;bar 1&quot;}</code>（<code>foo</code>也应加上引号），因此JsonConverter在处理时会抛出异常，因此最终会输出到死信主题。</p> <p>但是只有看到消息才能知道它是无效的JSON，即便如此，也只能假设消息被拒绝的原因，要确定Kafka连接器将消息视为无效的实际原因，有两个方法：</p> <ul><li>死信队列的消息头；</li> <li>Kafka连接器的工作节点日志。</li></ul> <p>下面会分别介绍。</p> <h2 id="记录消息的失败原因：消息头"><a href="#记录消息的失败原因：消息头" aria-hidden="true" class="header-anchor">#</a> 记录消息的失败原因：消息头</h2> <p>消息头是使用Kafka消息的键、值和时间戳存储的附加元数据，是在Kafka 0.11版本中引入的。Kafka连接器可以将有关消息拒绝原因的信息写入消息本身的消息头中。这个做法比写入日志文件更好，因为它将原因直接与消息联系起来。</p> <p>配置如下的参数，可以在死信队列的消息头中包含拒绝原因：</p> <div class="language-properties line-numbers-mode"><pre class="language-properties"><code><span class="token attr-name">errors.deadletterqueue.context.headers.enable</span> <span class="token punctuation">=</span> <span class="token attr-value">true</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>配置示例大致如下：</p> <div class="language-bash line-numbers-mode"><pre class="language-bash"><code><span class="token function">curl</span> -X POST http://localhost:8083/connectors -H <span class="token string">&quot;Content-Type: application/json&quot;</span> -d <span class="token string">'{
        &quot;name&quot;: &quot;file_sink_03&quot;,
        &quot;config&quot;: {
                &quot;connector.class&quot;: &quot;org.apache.kafka.connect.file.FileStreamSinkConnector&quot;,
                &quot;topics&quot;:&quot;test_topic_json&quot;,
                &quot;value.converter&quot;:&quot;org.apache.kafka.connect.json.JsonConverter&quot;,
                &quot;value.converter.schemas.enable&quot;: false,
                &quot;key.converter&quot;:&quot;org.apache.kafka.connect.json.JsonConverter&quot;,
                &quot;key.converter.schemas.enable&quot;: false,
                &quot;file&quot;: &quot;/data/file_sink_03.txt&quot;,
                &quot;errors.tolerance&quot;: &quot;all&quot;,
                &quot;errors.deadletterqueue.topic.name&quot;:&quot;dlq_file_sink_03&quot;,
                &quot;errors.deadletterqueue.topic.replication.factor&quot;: 1,
                &quot;errors.deadletterqueue.context.headers.enable&quot;:true
                }
        }'</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br></div></div><p>和之前一致，连接器可以正常运行（因为配置了<code>errors.tolerance=all</code>）。</p> <div class="language-bash line-numbers-mode"><pre class="language-bash"><code>$ <span class="token function">curl</span> -s <span class="token string">&quot;http://localhost:8083/connectors/file_sink_03/status&quot;</span><span class="token operator">|</span> <span class="token punctuation">\</span>
    jq -c -M <span class="token string">'[.name,.tasks[].state]'</span>
<span class="token punctuation">[</span><span class="token string">&quot;file_sink_03&quot;</span>,<span class="token string">&quot;RUNNING&quot;</span><span class="token punctuation">]</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p>源主题中的有效消息会正常写入目标文件：</p> <div class="language-bash line-numbers-mode"><pre class="language-bash"><code>$ <span class="token function">head</span> data/file_sink_03.txt
<span class="token punctuation">{</span>foo<span class="token operator">=</span>bar <span class="token number">1</span><span class="token punctuation">}</span>
<span class="token punctuation">{</span>foo<span class="token operator">=</span>bar <span class="token number">2</span><span class="token punctuation">}</span>
<span class="token punctuation">{</span>foo<span class="token operator">=</span>bar <span class="token number">3</span><span class="token punctuation">}</span>
<span class="token punctuation">[</span>…<span class="token punctuation">]</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><p>可以使用任何消费者工具来检查死信队列上的消息（之前使用了KSQL），不过这里会使用kafkacat，然后马上就会看到原因，最简单的操作大致如下：</p> <div class="language-bash line-numbers-mode"><pre class="language-bash"><code>kafkacat -b localhost:9092 -t dlq_file_sink_03
% Auto-selecting Consumer mode <span class="token punctuation">(</span>use -P or -C to override<span class="token punctuation">)</span>
<span class="token punctuation">{</span>foo:<span class="token string">&quot;bar 1&quot;</span><span class="token punctuation">}</span>
<span class="token punctuation">{</span>foo:<span class="token string">&quot;bar 2&quot;</span><span class="token punctuation">}</span>
…
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><p>不过kafkacat有更强大的功能，可以看到比消息本身更多的信息：</p> <div class="language-bash line-numbers-mode"><pre class="language-bash"><code>kafkacat -b localhost:9092 -t dlq_file_sink_03 -C -o-1 -c1 <span class="token punctuation">\</span>
  -f <span class="token string">'<span class="token entity" title="\n">\n</span>Key (%K bytes): %k
  Value (%S bytes): %s
  Timestamp: %T
  Partition: %p
  Offset: %o
  Headers: %h<span class="token entity" title="\n">\n</span>'</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br></div></div><p>这个命令将获取最后一条消息（<code>-o-1</code>，针对偏移量，使用最后一条消息），只读取一条消息（<code>-c1</code>），并且通过<code>-f</code>参数对其进行格式化，以更易于理解：</p> <div class="language- line-numbers-mode"><pre class="language-text"><code>Key (-1 bytes):
  Value (13 bytes): {foo:&quot;bar 5&quot;}
  Timestamp: 1548350164096
  Partition: 0
  Offset: 34
  Headers: __connect.errors.topic=test_topic_json,__connect.errors.partition=0,__connect.errors.offset=94,__connect.errors.connector.name=file_sink_03,__connect.errors.task.id=0,__connect.errors.stage=VALU
E_CONVERTER,__connect.errors.class.name=org.apache.kafka.connect.json.JsonConverter,__connect.errors.exception.class.name=org.apache.kafka.connect.errors.DataException,__connect.errors.exception.message=Co
nverting byte[] to Kafka Connect data failed due to serialization error: ,__connect.errors.exception.stacktrace=org.apache.kafka.connect.errors.DataException: Converting byte[] to Kafka Connect data failed
 due to serialization error:
[…]
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br></div></div><p>也可以只显示消息头，并使用一些简单的技巧将其拆分，这样可以更清楚地看到该问题的更多信息：</p> <div class="language- line-numbers-mode"><pre class="language-text"><code>$ kafkacat -b localhost:9092 -t dlq_file_sink_03 -C -o-1 -c1 -f '%h'|tr ',' '\n'
__connect.errors.topic=test_topic_json
__connect.errors.partition=0
__connect.errors.offset=94
__connect.errors.connector.name=file_sink_03
__connect.errors.task.id=0
__connect.errors.stage=VALUE_CONVERTER
__connect.errors.class.name=org.apache.kafka.connect.json.JsonConverter
__connect.errors.exception.class.name=org.apache.kafka.connect.errors.DataException
__connect.errors.exception.message=Converting byte[] to Kafka Connect data failed due to serialization error:
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br></div></div><p>Kafka连接器处理的每条消息都来自源主题和该主题中的特定点（偏移量），消息头已经准确地说明了这一点。因此可以使用它来回到原始主题并在需要时检查原始消息，由于死信队列已经有一个消息的副本，这个检查更像是一个保险的做法。</p> <p>根据从上面的消息头中获取的详细信息，可以再检查一下源消息：</p> <div class="language- line-numbers-mode"><pre class="language-text"><code>__connect.errors.topic=test_topic_json
__connect.errors.offset=94
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>将这些值分别插入到kafkacat的代表主题和偏移的<code>-t</code>和<code>-o</code>参数中，可以得到：</p> <div class="language-bash line-numbers-mode"><pre class="language-bash"><code>$ kafkacat -b localhost:9092 -C <span class="token punctuation">\</span>
  -t test_topic_json -o94 <span class="token punctuation">\</span>
  -f <span class="token string">'<span class="token entity" title="\n">\n</span>Key (%K bytes): %k
  Value (%S bytes): %s
  Timestamp: %T
  Partition: %p
  Offset: %o
  Topic: %t<span class="token entity" title="\n">\n</span>'</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br></div></div><div class="language- line-numbers-mode"><pre class="language-text"><code>Key (-1 bytes):
  Value (13 bytes): {foo:&quot;bar 5&quot;}
  Timestamp: 1548350164096
  Partition: 0
  Offset: 94
  Topic: test_topic_json
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br></div></div><p>与死信队列中的上述消息相比，可以看到完全相同，甚至包括时间戳，唯一的区别是主题、偏移量和消息头。</p> <h2 id="记录消息的失败原因：日志"><a href="#记录消息的失败原因：日志" aria-hidden="true" class="header-anchor">#</a> 记录消息的失败原因：日志</h2> <p>记录消息的拒绝原因的第二个选项是将其写入日志。根据安装方式不同，Kafka连接器会将其写入标准输出或日志文件。无论哪种方式都会为每个失败的消息生成一堆详细输出。进行如下配置可启用此功能：</p> <div class="language-properties line-numbers-mode"><pre class="language-properties"><code><span class="token attr-name">errors.log.enable</span> <span class="token punctuation">=</span> <span class="token attr-value">true</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>通过配置<code>errors.log.include.messages = true</code>，还可以在输出中包含有关消息本身的元数据。此元数据中包括一些和上面提到的消息头中一样的项目，包括源消息的主题和偏移量。注意它不包括消息键或值本身。</p> <p>这时的连接器配置如下：</p> <div class="language-bash line-numbers-mode"><pre class="language-bash"><code><span class="token function">curl</span> -X POST http://localhost:8083/connectors -H <span class="token string">&quot;Content-Type: application/json&quot;</span> -d <span class="token string">'{
        &quot;name&quot;: &quot;file_sink_04&quot;,
        &quot;config&quot;: {
                &quot;connector.class&quot;: &quot;org.apache.kafka.connect.file.FileStreamSinkConnector&quot;,
                &quot;topics&quot;:&quot;test_topic_json&quot;,
                &quot;value.converter&quot;:&quot;org.apache.kafka.connect.json.JsonConverter&quot;,
                &quot;value.converter.schemas.enable&quot;: false,
                &quot;key.converter&quot;:&quot;org.apache.kafka.connect.json.JsonConverter&quot;,
                &quot;key.converter.schemas.enable&quot;: false,
                &quot;file&quot;: &quot;/data/file_sink_04.txt&quot;,
                &quot;errors.tolerance&quot;: &quot;all&quot;,
                &quot;errors.log.enable&quot;:true,
                &quot;errors.log.include.messages&quot;:true
                }
        }'</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br></div></div><p>连接器是可以成功运行的：</p> <div class="language-bash line-numbers-mode"><pre class="language-bash"><code>$ <span class="token function">curl</span> -s <span class="token string">&quot;http://localhost:8083/connectors/file_sink_04/status&quot;</span><span class="token operator">|</span> <span class="token punctuation">\</span>
    jq -c -M <span class="token string">'[.name,.tasks[].state]'</span>
<span class="token punctuation">[</span><span class="token string">&quot;file_sink_04&quot;</span>,<span class="token string">&quot;RUNNING&quot;</span><span class="token punctuation">]</span>
Valid records from the <span class="token builtin class-name">source</span> topic get written to the target file:
$ <span class="token function">head</span> data/file_sink_04.txt
<span class="token punctuation">{</span>foo<span class="token operator">=</span>bar <span class="token number">1</span><span class="token punctuation">}</span>
<span class="token punctuation">{</span>foo<span class="token operator">=</span>bar <span class="token number">2</span><span class="token punctuation">}</span>
<span class="token punctuation">{</span>foo<span class="token operator">=</span>bar <span class="token number">3</span><span class="token punctuation">}</span>
<span class="token punctuation">[</span>…<span class="token punctuation">]</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br></div></div><p>这时去看Kafka连接器的工作节点日志，会发现每个失败的消息都有错误记录：</p> <div class="language- line-numbers-mode"><pre class="language-text"><code>ERROR Error encountered in task file_sink_04-0. Executing stage 'VALUE_CONVERTER' with class 'org.apache.kafka.connect.json.JsonConverter', where consumed record is {topic='test_topic_json', partition=0, offset=94, timestamp=1548350164096, timestampType=CreateTime}. (org.apache.kafka.connect.runtime.errors.LogReporter)
org.apache.kafka.connect.errors.DataException: Converting byte[] to Kafka Connect data failed due to serialization error:
 at org.apache.kafka.connect.json.JsonConverter.toConnectData(JsonConverter.java:334)
[…]
Caused by: org.apache.kafka.common.errors.SerializationException: com.fasterxml.jackson.core.JsonParseException: Unexpected character ('f' (code 102)): was expecting double-quote to start field name
 at [Source: (byte[])&quot;{foo:&quot;bar 5&quot;}&quot;; line: 1, column: 3]
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br></div></div><p>可以看到错误本身，还有就是和错误有关的信息：</p> <div class="language- line-numbers-mode"><pre class="language-text"><code>{topic='test_topic_json', partition=0, offset=94, timestamp=1548350164096, timestampType=CreateTime}
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br></div></div><p>如上所示，可以在kafkacat等工具中使用该主题和偏移量来检查源主题上的消息。根据抛出的异常也可能会看到记录的源消息：</p> <div class="language- line-numbers-mode"><pre class="language-text"><code>Caused by: org.apache.kafka.common.errors.SerializationException:
…
at [Source: (byte[])&quot;{foo:&quot;bar 5&quot;}&quot;; line: 1, column: 3]
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><h2 id="处理死信队列的消息"><a href="#处理死信队列的消息" aria-hidden="true" class="header-anchor">#</a> 处理死信队列的消息</h2> <p>虽然设置了一个死信队列，但是如何处理那些“死信”呢？因为它只是一个Kafka主题，所以可以像使用任何其它主题一样使用标准的Kafka工具。上面已经看到了，比如可以使用kafkacat来检查消息头，并且对于消息的内容及其元数据的一般检查kafkacat也可以做。当然根据被拒绝的原因，也可以选择对消息进行重播。</p> <p>一个场景是连接器正在使用Avro转换器，但是主题上的却是JSON格式消息（因此被写入死信队列）。可能由于遗留原因JSON和Avro格式的生产者都在写入源主题，这个问题得解决，但是目前只需要将管道流中的数据写入接收器即可。</p> <p>首先，从初始的接收器读取源主题开始，使用Avro反序列化并路由到死信队列：</p> <div class="language-bash line-numbers-mode"><pre class="language-bash"><code><span class="token function">curl</span> -X POST http://localhost:8083/connectors -H <span class="token string">&quot;Content-Type: application/json&quot;</span> -d <span class="token string">'{
        &quot;name&quot;: &quot;file_sink_06__01-avro&quot;,
        &quot;config&quot;: {
                &quot;connector.class&quot;: &quot;org.apache.kafka.connect.file.FileStreamSinkConnector&quot;,
                &quot;topics&quot;:&quot;test_topic_avro&quot;,
                &quot;file&quot;:&quot;/data/file_sink_06.txt&quot;,
                &quot;key.converter&quot;: &quot;io.confluent.connect.avro.AvroConverter&quot;,
                &quot;key.converter.schema.registry.url&quot;: &quot;http://schema-registry:8081&quot;,
                &quot;value.converter&quot;: &quot;io.confluent.connect.avro.AvroConverter&quot;,
                &quot;value.converter.schema.registry.url&quot;: &quot;http://schema-registry:8081&quot;,
                &quot;errors.tolerance&quot;:&quot;all&quot;,
                &quot;errors.deadletterqueue.topic.name&quot;:&quot;dlq_file_sink_06__01&quot;,
                &quot;errors.deadletterqueue.topic.replication.factor&quot;:1,
                &quot;errors.deadletterqueue.context.headers.enable&quot;:true,
                &quot;errors.retry.delay.max.ms&quot;: 60000,
                &quot;errors.retry.timeout&quot;: 300000
                }
        }'</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br></div></div><p>另外再创建第二个接收器，将第一个接收器的死信队列作为源主题，并尝试将记录反序列化为JSON，在这里要更改的是<code>value.converter</code>、<code>key.converter</code>、源主题名和死信队列名（如果此连接器需要将任何消息路由到死信队列，要避免递归）。</p> <p><img src="https://www.confluent.io/wp-content/uploads/Create_Second_Sink-e1552340041115.png" alt=""></p> <div class="language-bash line-numbers-mode"><pre class="language-bash"><code><span class="token function">curl</span> -X POST http://localhost:8083/connectors -H <span class="token string">&quot;Content-Type: application/json&quot;</span> -d <span class="token string">'{
        &quot;name&quot;: &quot;file_sink_06__02-json&quot;,
        &quot;config&quot;: {
                &quot;connector.class&quot;: &quot;org.apache.kafka.connect.file.FileStreamSinkConnector&quot;,
                &quot;topics&quot;:&quot;dlq_file_sink_06__01&quot;,
                &quot;file&quot;:&quot;/data/file_sink_06.txt&quot;,
                &quot;value.converter&quot;:&quot;org.apache.kafka.connect.json.JsonConverter&quot;,
                &quot;value.converter.schemas.enable&quot;: false,
                &quot;key.converter&quot;:&quot;org.apache.kafka.connect.json.JsonConverter&quot;,
                &quot;key.converter.schemas.enable&quot;: false,
                &quot;errors.tolerance&quot;:&quot;all&quot;,
                &quot;errors.deadletterqueue.topic.name&quot;:&quot;dlq_file_sink_06__02&quot;,
                &quot;errors.deadletterqueue.topic.replication.factor&quot;:1,
                &quot;errors.deadletterqueue.context.headers.enable&quot;:true,
                &quot;errors.retry.delay.max.ms&quot;: 60000,
                &quot;errors.retry.timeout&quot;: 300000
                }
        }'</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br></div></div><p>现在可以验证一下。</p> <p>首先，源主题收到20条Avro消息，之后可以看到20条消息被读取并被原始Avro接收器接收：</p> <p><img src="https://www.confluent.io/wp-content/uploads/Source_Records_Read_Avro_Sink_Records_Written-e1552335642875.png" alt=""></p> <p>然后发送8条JSON消息，这时8条消息被发送到死信队列，然后被JSON接收器接收：</p> <p><img src="https://www.confluent.io/wp-content/uploads/JSON_Records_Messages_DLQ_JSON_Sink-e1552335802462.png" alt=""></p> <p>现在再发送5条格式错误的JSON消息，之后可以看到两者都有失败的消息，有2点可以确认：</p> <ol><li>从Avro接收器发送到死信队列的消息数与成功发送的JSON消息数之间有差异；</li> <li>消息被发送到JSON接收器的死信队列。</li></ol> <p><img src="https://www.confluent.io/wp-content/uploads/Records_Requests-e1552335946185.png" alt=""></p> <h2 id="通过ksql监控死信队列"><a href="#通过ksql监控死信队列" aria-hidden="true" class="header-anchor">#</a> 通过KSQL监控死信队列</h2> <p>除了使用JMX监控死信队列之外，还可以利用KSQL的聚合能力编写一个简单的流应用来监控消息写入队列的速率：</p> <div class="language-sql line-numbers-mode"><pre class="language-sql"><code><span class="token comment">-- 为每个死信队列主题注册流。</span>
<span class="token keyword">CREATE</span> STREAM dlq_file_sink_06__01（MSG <span class="token keyword">VARCHAR</span>）<span class="token keyword">WITH</span>（KAFKA_TOPIC <span class="token operator">=</span><span class="token string">'dlq_file_sink_06__01'</span>，VALUE_FORMAT <span class="token operator">=</span><span class="token string">'DELIMITED'</span>）<span class="token punctuation">;</span>
<span class="token keyword">CREATE</span> STREAM dlq_file_sink_06__02（MSG <span class="token keyword">VARCHAR</span>）<span class="token keyword">WITH</span>（KAFKA_TOPIC <span class="token operator">=</span><span class="token string">'dlq_file_sink_06__02'</span>，VALUE_FORMAT <span class="token operator">=</span><span class="token string">'DELIMITED'</span>）<span class="token punctuation">;</span>

<span class="token comment">-- 从主题的开头消费数据</span>
<span class="token keyword">SET</span> <span class="token string">'auto.offset.reset'</span> <span class="token operator">=</span> <span class="token string">'earliest'</span><span class="token punctuation">;</span>

<span class="token comment">-- 使用其它列创建监控流，可用于后续聚合查询</span>
<span class="token keyword">CREATE</span> STREAM DLQ_MONITOR <span class="token keyword">WITH</span> <span class="token punctuation">(</span>VALUE_FORMAT<span class="token operator">=</span><span class="token string">'AVRO'</span><span class="token punctuation">)</span> <span class="token keyword">AS</span> \
  <span class="token keyword">SELECT</span> <span class="token string">'dlq_file_sink_06__01'</span> <span class="token keyword">AS</span> SINK_NAME<span class="token punctuation">,</span> \
         <span class="token string">'Records: '</span> <span class="token keyword">AS</span> GROUP_COL<span class="token punctuation">,</span> \
         MSG \
    <span class="token keyword">FROM</span> dlq_file_sink_06__01<span class="token punctuation">;</span>

<span class="token comment">-- 使用来自第二个死信队列的消息注入相同的监控流</span>
<span class="token keyword">INSERT</span> <span class="token keyword">INTO</span> DLQ_MONITOR \
  <span class="token keyword">SELECT</span> <span class="token string">'dlq_file_sink_06__02'</span> <span class="token keyword">AS</span> SINK_NAME<span class="token punctuation">,</span> \
         <span class="token string">'Records: '</span> <span class="token keyword">AS</span> GROUP_COL<span class="token punctuation">,</span> \
         MSG \
    <span class="token keyword">FROM</span> dlq_file_sink_06__02<span class="token punctuation">;</span>

<span class="token comment">-- 在每个死信队列每分钟的时间窗口内，创建消息的聚合视图</span>
<span class="token keyword">CREATE</span> <span class="token keyword">TABLE</span> DLQ_MESSAGE_COUNT_PER_MIN <span class="token keyword">AS</span> \
  <span class="token keyword">SELECT</span> TIMESTAMPTOSTRING<span class="token punctuation">(</span>WINDOWSTART<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token string">'yyyy-MM-dd HH:mm:ss'</span><span class="token punctuation">)</span> <span class="token keyword">AS</span> START_TS<span class="token punctuation">,</span> \
         SINK_NAME<span class="token punctuation">,</span> \
         GROUP_COL<span class="token punctuation">,</span> \
         <span class="token function">COUNT</span><span class="token punctuation">(</span><span class="token operator">*</span><span class="token punctuation">)</span> <span class="token keyword">AS</span> DLQ_MESSAGE_COUNT \
    <span class="token keyword">FROM</span> DLQ_MONITOR \
          WINDOW TUMBLING <span class="token punctuation">(</span>SIZE <span class="token number">1</span> <span class="token keyword">MINUTE</span><span class="token punctuation">)</span> \
 <span class="token keyword">GROUP</span> <span class="token keyword">BY</span> SINK_NAME<span class="token punctuation">,</span> \
          GROUP_COL<span class="token punctuation">;</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br><span class="line-number">27</span><br><span class="line-number">28</span><br><span class="line-number">29</span><br><span class="line-number">30</span><br><span class="line-number">31</span><br></div></div><p>这个聚合表可以以交互式的方式进行查询，下面显示了一分钟内每个死信队列中的消息数量：</p> <div class="language- line-numbers-mode"><pre class="language-text"><code>ksql&gt; SELECT START_TS, SINK_NAME, DLQ_MESSAGE_COUNT FROM DLQ_MESSAGE_COUNT_PER_MIN;
2019-02-01 02:56:00 | dlq_file_sink_06__01 | 9
2019-02-01 03:10:00 | dlq_file_sink_06__01 | 8
2019-02-01 03:12:00 | dlq_file_sink_06__01 | 5
2019-02-01 02:56:00 | dlq_file_sink_06__02 | 5
2019-02-01 03:12:00 | dlq_file_sink_06__02 | 5
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br></div></div><p>因为这个表的下面是Kafka主题，所以可以将其路由到期望的任何监控仪表盘，还可以用于驱动告警。假定有几条错误消息是可以接受的，但是一分钟内超过5条消息就是个大问题需要关注：</p> <div class="language-sql line-numbers-mode"><pre class="language-sql"><code><span class="token keyword">CREATE</span> <span class="token keyword">TABLE</span> DLQ_BREACH <span class="token keyword">AS</span> \
    <span class="token keyword">SELECT</span> START_TS<span class="token punctuation">,</span> SINK_NAME<span class="token punctuation">,</span> DLQ_MESSAGE_COUNT \
      <span class="token keyword">FROM</span> DLQ_MESSAGE_COUNT_PER_MIN \
     <span class="token keyword">WHERE</span> DLQ_MESSAGE_COUNT<span class="token operator">&gt;</span><span class="token number">5</span><span class="token punctuation">;</span>
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><p>现在又有了一个报警服务可以订阅的<code>DLQ_BREACH</code>主题，当收到任何消息时，可以触发适当的操作（例如通知）。</p> <div class="language- line-numbers-mode"><pre class="language-text"><code>ksql&gt; SELECT START_TS, SINK_NAME, DLQ_MESSAGE_COUNT FROM DLQ_BREACH;
2019-02-01 02:56:00 | dlq_file_sink_06__01 | 9
2019-02-01 03:10:00 | dlq_file_sink_06__01 | 8
</code></pre> <div class="line-numbers-wrapper"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><h2 id="kafka连接器哪里没有提供错误处理？"><a href="#kafka连接器哪里没有提供错误处理？" aria-hidden="true" class="header-anchor">#</a> Kafka连接器哪里没有提供错误处理？</h2> <p>Kafka连接器的错误处理方式，如下表所示：</p> <table><thead><tr><th>连接器生命周期阶段</th> <th>描述</th> <th>是否处理错误？</th></tr></thead> <tbody><tr><td>开始</td> <td>首次启动连接器时，其将执行必要的初始化，例如连接到数据存储</td> <td>无</td></tr> <tr><td>拉取（针对源连接器）</td> <td>从源数据存储读取消息</td> <td>无</td></tr> <tr><td>格式转换</td> <td>从Kafka主题读写数据并对JSON/Avro格式进行序列化/反序列化</td> <td>有</td></tr> <tr><td>单消息转换</td> <td>应用任何已配置的单消息转换</td> <td>有</td></tr> <tr><td>接收（针对接收连接器）</td> <td>将消息写入目标数据存储</td> <td>无</td></tr></tbody></table> <p>注意源连接器没有死信队列。</p> <h2 id="错误处理配置流程"><a href="#错误处理配置流程" aria-hidden="true" class="header-anchor">#</a> 错误处理配置流程</h2> <p>关于连接器错误处理的配置，可以按照如下的流程一步步进阶：</p> <p><img src="https://www.confluent.io/wp-content/uploads/Permutations_Error_Handling_Kafka_Connect_Configuration.png" alt=""></p> <h2 id="总结"><a href="#总结" aria-hidden="true" class="header-anchor">#</a> 总结</h2> <p>处理错误是任何稳定可靠的数据管道的重要组成部分，根据数据的使用方式，可以有两个选项。如果管道任何错误的消息都不能接受，表明上游存在严重的问题，那么就应该立即停止处理（这是Kafka连接器的默认行为）。</p> <p>另一方面，如果只是想将数据流式传输到存储以进行分析或非关键性处理，那么只要不传播错误，保持管道稳定运行则更为重要。这时就可以定义错误的处理方式，推荐的方式是使用死信队列并密切监视来自Kafka连接器的可用JMX指标。</p></div> <footer class="page-edit"><!----> <div class="last-updated"><span class="prefix">最后更新时间：:</span> <span class="time">4/13/2019, 9:24:12 AM</span></div></footer> <div class="page-nav"><p class="inner"><span class="prev">
      ←
      <a href="/confluent/Kafka-ConvertersSerialization.html" class="prev">Kafka连接器深度解读之转换器和序列化</a></span> <span class="next"><a href="/confluent/Kafka-ConnectImprovementsIn-2-3.html">Kafka连接器之在2.3版本中的改进</a>
      →
    </span></p></div> </main></div><div class="global-ui"></div></div>
    <script src="/assets/js/app.9949b77c.js" defer></script><script src="/assets/js/2.6817020a.js" defer></script><script src="/assets/js/13.a93a44d2.js" defer></script>
  </body>
</html>
